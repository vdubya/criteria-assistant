{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da04e53c",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/vdubya/criteria-assistant/blob/main/src/UFC%20-%20Parse%20Structure.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df980194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 0.1.1  \n",
    "# Chunk 1 - Setup and PDF Parsing\n",
    "!pip install requests pdfplumber\n",
    "\n",
    "import os, re, csv, json, zipfile, glob, itertools, string, traceback, requests, pdfplumber\n",
    "\n",
    "CLEAN = True\n",
    "LOOKAHEAD = 20\n",
    "DEBUG = 2\n",
    "BAD_TOP_LINES = {\"CHAPTER 2 PRELIMINARY DESIGN DATA\"}\n",
    "ZIP_FILE, ZIP_DIR = \"ufc.zip\", \"ufc_zip\"\n",
    "SINGLE_URL = \"https://www.wbdg.org/FFC/DOD/UFC/ufc_1_300_01_2021_c1.pdf\"\n",
    "SINGLE_PDF = os.path.basename(SINGLE_URL)\n",
    "COMB_CSV, COMB_JSON, COMB_TREE = \"combined_hierarchy_all.csv\", \"combined_hierarchy_flat.json\", \"combined_hierarchy_tree.json\"\n",
    "\n",
    "CH_RE = re.compile(r\"^CHAPTER\\s+(\\d+)\\s+(.+)$\", re.I)\n",
    "AP_RE = re.compile(r\"^APPENDIX\\s+([A-Z])\\s+(.+)$\", re.I)\n",
    "SEC_RE = re.compile(r\"^(\\d+(?:-\\d+)+(?:\\.\\d+)*)\\s+(.+)$\", re.I)  # strict 1-n hierarchy only\n",
    "DATE_RE = re.compile(r\"\\d{1,2}\\s+[A-Za-z]{3,9}\\s+\\d{4}\")\n",
    "SENT_RE = re.compile(r\"(?<=[.!?])\\s+\")\n",
    "level_from = lambda n: n.count(\".\") + 2\n",
    "\n",
    "def dbg(level, msg):\n",
    "    if DEBUG >= level:\n",
    "        print(msg)\n",
    "\n",
    "if CLEAN:\n",
    "    patterns_to_delete = [\"*.csv\", \"*.json\"]\n",
    "    files_to_delete = []\n",
    "    for pat in patterns_to_delete:\n",
    "        files_to_delete.extend(glob.glob(pat))\n",
    "    print(\"\ud83d\uddd1\ufe0f Files to delete:\", files_to_delete)\n",
    "    for f in files_to_delete:\n",
    "        os.remove(f)\n",
    "    print(\"\ud83d\uddd1\ufe0f  All CSV and JSON files deleted.\")\n",
    "\n",
    "def download(url, dst):\n",
    "    if not os.path.exists(dst):\n",
    "        print(f\"\u2b07\ufe0f  Downloading: {url}\")\n",
    "        with requests.get(url, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            with open(dst, \"wb\") as f:\n",
    "                for chunk in r.iter_content(8192):\n",
    "                    f.write(chunk)\n",
    "        print(f\"\u2705 Saved: {dst}\")\n",
    "    else:\n",
    "        print(f\"\u2705 Using cached: {dst}\")\n",
    "\n",
    "def get_pdfs():\n",
    "    if os.path.exists(ZIP_FILE):\n",
    "        if not os.path.isdir(ZIP_DIR):\n",
    "            print(\"\ud83d\udce6 Extracting ufc.zip \u2026\")\n",
    "            with zipfile.ZipFile(ZIP_FILE) as z:\n",
    "                z.extractall(ZIP_DIR)\n",
    "        all_pdfs = [p for p in glob.glob(f\"{ZIP_DIR}/**/*.pdf\", recursive=True)\n",
    "                     if \"/Reference/\" not in p and \"\\\\Reference\\\\\" not in p]\n",
    "        ufc_pdfs = sorted([p for p in all_pdfs if os.path.basename(p).lower().startswith(\"ufc_\")])\n",
    "        other_pdfs = sorted([p for p in all_pdfs if not os.path.basename(p).lower().startswith(\"ufc_\")])\n",
    "        pdfs = ufc_pdfs + other_pdfs\n",
    "        print(f\"\ud83d\uddc2\ufe0f  Found {len(pdfs):,} PDFs to process (UFCs first).\")\n",
    "        return pdfs\n",
    "    download(SINGLE_URL, SINGLE_PDF)\n",
    "    return [SINGLE_PDF]\n",
    "\n",
    "def meta_from_pdf(path, url=\"\"):\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        first_page_text = pdf.pages[0].extract_text() or \"\"\n",
    "    lines = [ln.strip() for ln in first_page_text.splitlines() if ln.strip()]\n",
    "    title = next((ln for ln in lines if ln.isupper()), \"UNKNOWN TITLE\")\n",
    "    date = next((m.group(0) for ln in lines if (m := DATE_RE.search(ln))), \"UNKNOWN DATE\")\n",
    "    return {\"file_name\": os.path.basename(path), \"source_url\": url, \"ufc_title\": title, \"issue_date\": date}\n",
    "\n",
    "def pdf_lines(path):\n",
    "    lines = []\n",
    "    with pdfplumber.open(path) as pdf:\n",
    "        for pg, page in enumerate(pdf.pages, 1):\n",
    "            raw_lines = (page.extract_text() or \"\").splitlines()\n",
    "            clean_lines = raw_lines[2:]\n",
    "            if clean_lines and clean_lines[0].strip().lower().startswith(\"change\"):\n",
    "                clean_lines = clean_lines[1:]\n",
    "            if clean_lines and clean_lines[-1].strip().isdigit():\n",
    "                clean_lines = clean_lines[:-1]\n",
    "            for ln in clean_lines:\n",
    "                lines.append({\"text\": ln.rstrip(), \"pdf_page\": pg})\n",
    "    dbg(2, f\"\u2705 Processed {len(lines)} lines from {path}\")\n    return lines\n",
    "\n",
    "def locate_body(ls):\n",
    "    for i, l in enumerate(ls):\n",
    "        line_text = l[\"text\"].strip().upper()\n",
    "        if line_text == \"CHAPTER 1 INTRODUCTION\" and \"...\" not in l[\"text\"]:\n",
    "            print(f\"\u2705 Found 'CHAPTER 1 INTRODUCTION' at line {i}\")\n",
    "            return i, False\n",
    "    fallback_markers = [\"1-1 BACKGROUND.\", \"1-1 PURPOSE AND SCOPE.\"]\n",
    "    for i, l in enumerate(ls):\n",
    "        line_text = l[\"text\"].strip().upper()\n",
    "        if line_text in fallback_markers and \"...\" not in l[\"text\"]:\n",
    "            print(f\"\u26a0\ufe0f Using fallback marker '{line_text}' at line {i}\")\n",
    "            return i, True\n",
    "    raise RuntimeError(\"Body start not found: Neither 'CHAPTER 1 INTRODUCTION' nor fallback 1-1 markers found outside TOC!\")\n",
    "\n",
    "def to_paragraphs(raw):\n",
    "    paras, buf, prev = [], [], \"\"\n",
    "    def flush():\n",
    "        if buf:\n",
    "            paras.append(\" \".join(buf).strip())\n",
    "            buf.clear()\n",
    "    for ln in raw:\n",
    "        t = ln.rstrip()\n",
    "        if t == \"\": flush(); prev = t; continue\n",
    "        if t.lstrip().startswith((\"\u2022\", \"-\", \"\u2014\")): flush(); buf.append(t); prev = t; continue\n",
    "        if prev.endswith(\".\") and t and t[0] in string.ascii_uppercase: flush()\n",
    "        buf.append(t); prev = t\n",
    "    flush()\n",
    "    return paras\n",
    "\n",
    "def sentences(paras):\n",
    "    return [[s.strip() for s in SENT_RE.split(p) if s.strip()] for p in paras]\n",
    "\n",
    "def parse_pdf(path):\n",
    "    dbg(1, f\"\ud83d\udd0d Parsing: {path}\")\n",
    "    ls = pdf_lines(path)\n",
    "    start, is_fallback = locate_body(ls)\n",
    "    rows = []\n",
    "    paragraphs = []\n",
    "    def flush_paragraphs():\n",
    "        nonlocal paragraphs\n",
    "        if paragraphs and rows:\n",
    "            paras = to_paragraphs(paragraphs)\n",
    "            sents = sentences(paras)\n",
    "            rows[-1][\"sentences\"] = sents\n",
    "            paragraphs = []\n",
    "    logical_offset = None\n",
    "\n",
    "    if is_fallback:\n",
    "        logical_offset = ls[start][\"pdf_page\"] - 1  # FIX: fallback sets logical_offset\n",
    "        rows.append({\n",
    "            \"level\": 1,\n",
    "            \"number\": \"CHAPTER 1\",\n",
    "            \"title\": \"INTRODUCTION\",\n",
    "            \"parent\": \"\",\n",
    "            \"pdf_page_start\": ls[start][\"pdf_page\"],\n",
    "            \"logical_page_start\": 1,\n",
    "            \"sentences\": []\n",
    "        })\n",
    "        print(\"\u26a0\ufe0f Injected 'CHAPTER 1 INTRODUCTION' due to fallback marker. Logical offset set.\")\n",
    "\n",
    "    for idx in range(start, len(ls)):\n",
    "        txt, pg = ls[idx][\"text\"], ls[idx][\"pdf_page\"]\n",
    "        st = txt.strip()\n",
    "        if logical_offset is None and st.upper() == \"CHAPTER 1 INTRODUCTION\":\n",
    "            logical_offset = pg - 1\n",
    "            dbg(2, f\"\u2705 logical_offset set to {logical_offset} on line {idx}\")\n",
    "        lp = pg - logical_offset if logical_offset is not None else pg\n",
    "        if st.upper() in BAD_TOP_LINES:\n",
    "            continue\n",
    "        flush_paragraphs()\n",
    "        if (m := CH_RE.match(txt)):\n",
    "            rows.append({\"level\": 1, \"number\": f\"CHAPTER {m.group(1)}\", \"title\": m.group(2).strip(),\n",
    "                         \"parent\": \"\", \"pdf_page_start\": pg, \"logical_page_start\": lp, \"sentences\": []})\n",
    "            continue\n",
    "        flush_paragraphs()\n",
    "        if (m := SEC_RE.match(txt)):\n",
    "            parent = \"\"\n",
    "            for r in reversed(rows):\n",
    "                if r[\"level\"] == 1:\n",
    "                    parent = r[\"number\"]\n",
    "                    break\n",
    "            rows.append({\"level\": 2, \"number\": m.group(1).strip(), \"title\": m.group(2).strip(),\n",
    "                         \"parent\": parent, \"pdf_page_start\": pg, \"logical_page_start\": lp, \"sentences\": []})\n",
    "            continue\n",
    "        if st and not st.endswith(\".\"):\n",
    "            paragraphs.append(st)\n",
    "        if st.endswith(\".\"):\n",
    "            paragraphs.append(st)\n",
    "            paras = to_paragraphs(paragraphs)\n",
    "            sents = sentences(paras)\n",
    "            rows[-1][\"sentences\"] = sents\n",
    "            paragraphs = []\n",
    "\n",
    "    flush_paragraphs()\n",
    "    dbg(1, f\"\u2705 Finished parsing {path}, structured hierarchy built.\")\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee70678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 0.1.0 - \n",
    "# Chunk 2 - Tree Building, Saving, and Output\n",
    "\n",
    "def build_tree(rows):\n",
    "    tree = []\n",
    "    stack = []\n",
    "    for r in rows:\n",
    "        node = r.copy()\n",
    "        node[\"children\"] = []\n",
    "        while stack and stack[-1][\"level\"] >= node[\"level\"]:\n",
    "            stack.pop()\n",
    "        if stack:\n",
    "            stack[-1][\"children\"].append(node)\n",
    "        else:\n",
    "            tree.append(node)\n",
    "        stack.append(node)\n",
    "    dbg(2, f\"\u2705 Tree structure built with {len(tree)} root nodes.\")\n",
    "    return tree\n",
    "\n",
    "def save_per_pdf(base, rows, meta):\n",
    "    try:\n",
    "        with open(f\"{base}_hierarchy_metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        with open(f\"{base}_hierarchy_flat.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(rows, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        nested_tree = build_tree(rows)\n",
    "        with open(f\"{base}_hierarchy_tree.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\"chapters\": nested_tree}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        with open(f\"{base}_hierarchy.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            w = csv.writer(f, quoting=csv.QUOTE_ALL)\n",
    "            w.writerow([\"GlobalID\", \"FileName\", \"Level\", \"Number\", \"Title\", \"Parent\",\n",
    "                         \"PDFPage\", \"LogicalPage\", \"SentencesJSON\"])\n",
    "            for r in rows:\n",
    "                w.writerow([r.get(\"global_id\", \"\"), meta[\"file_name\"], r[\"level\"], r[\"number\"], r[\"title\"],\n",
    "                             r[\"parent\"], r[\"pdf_page_start\"], r[\"logical_page_start\"],\n",
    "                             json.dumps(r[\"sentences\"], ensure_ascii=False)])\n",
    "        dbg(1, f\"\u2705 Saved hierarchy data for {base}\")\n",
    "    except Exception as e:\n",
    "        dbg(1, f\"\u274c Error in save_per_pdf for {base}: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "combined, meta_list, trees, gid = [], [], [], 1\n",
    "for pdf in get_pdfs():\n",
    "    try:\n",
    "        dbg(1, f\"\n",
    "\u25b6\ufe0f Processing: {pdf}\")\n",
    "        src = SINGLE_URL if pdf == SINGLE_PDF else \"\"\n",
    "        meta = meta_from_pdf(pdf, src)\n",
    "        dbg(1, f\"\u2139\ufe0f  Extracted metadata: {meta}\")\n",
    "        rows = parse_pdf(pdf)\n",
    "        if not rows:\n",
    "            dbg(1, f\"\u26a0\ufe0f  Warning: No heading entries found for {pdf}\")\n",
    "        for r in rows:\n",
    "            r[\"global_id\"] = gid\n",
    "            combined.append(r)\n",
    "            gid += 1\n",
    "        meta_list.append(meta)\n",
    "        save_per_pdf(os.path.splitext(os.path.basename(pdf))[0], rows, meta)\n",
    "        with open(f\"{os.path.splitext(os.path.basename(pdf))[0]}_hierarchy_tree.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "            t = json.load(f)[\"chapters\"]\n",
    "            trees.extend(t)\n",
    "    except Exception as e:\n",
    "        dbg(1, f\"\u26a0\ufe0f  Skipping {pdf} due to error: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "if combined:\n",
    "    dbg(1, \"\n",
    "\u2705 Writing combined outputs\u2026\")\n",
    "    with open(COMB_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f, quoting=csv.QUOTE_ALL)\n",
    "        w.writerow([\"GlobalID\", \"FileName\", \"Level\", \"Number\", \"Title\", \"Parent\",\n",
    "                     \"PDFPage\", \"LogicalPage\", \"SentencesJSON\"])\n",
    "        for r in combined:\n",
    "            w.writerow([r.get(\"global_id\", \"\"), meta[\"file_name\"], r[\"level\"], r[\"number\"], r[\"title\"],\n",
    "                         r[\"parent\"], r[\"pdf_page_start\"], r[\"logical_page_start\"],\n",
    "                         json.dumps(r[\"sentences\"], ensure_ascii=False)])\n",
    "    with open(COMB_JSON, \"w\", encoding=\"utf-8\") as jf:\n",
    "        json.dump(combined, jf, ensure_ascii=False, indent=2)\n",
    "    with open(COMB_TREE, \"w\", encoding=\"utf-8\") as jf:\n",
    "        nested_tree = build_tree(combined)\n",
    "        json.dump({\"bundle_metadata\": meta_list, \"chapters\": nested_tree},\n",
    "                  jf, ensure_ascii=False, indent=2)\n",
    "    dbg(1, f\"\u2705 Combined CSV \u2192 {COMB_CSV}\")\n",
    "    dbg(1, f\"\u2705 Combined JSON \u2192 {COMB_JSON}\")\n",
    "    dbg(1, f\"\u2705 Combined TREE \u2192 {COMB_TREE}\")\n",
    "else:\n",
    "    dbg(1, \"\u274c No PDFs parsed successfully; no combined files created.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyAutoGen",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
