{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMB3X1wZqCKj+3g4xZ0DA5X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vdubya/criteria-assistant/blob/main/src/UFC_DownloadAllWithMetadata_vFINAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "xKIamTIfME4y",
        "outputId": "a2aa661a-10df-4a03-c70e-28cf76035648"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.2)\n",
            "[INFO] Scraping UFC list from: https://www.wbdg.org/dod/ufc?field_status_value=1&field_series_value=All\n",
            "[INFO] Scraping UFC list from: https://www.wbdg.org/dod/ufc?field_status_value=1&field_series_value=All&page=1\n",
            "[INFO] Scraping UFC list from: https://www.wbdg.org/dod/ufc?field_status_value=2&field_series_value=All\n",
            "[INFO] Scraping UFC list from: https://www.wbdg.org/dod/ufc?field_status_value=3&field_series_value=All\n",
            "[INFO] Saving final JSON data...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Missing status for entry: UFC Complete",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a90cf57bb4a9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Missing status for entry: {entry.get('ufc_full_name')}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m             \u001b[0mstatus_lower\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus_lower\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"active\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"inactive\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"archived\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Missing status for entry: UFC Complete"
          ]
        }
      ],
      "source": [
        "# =======================\n",
        "# Step 0: Install dependencies\n",
        "# =======================\n",
        "!pip install requests beautifulsoup4\n",
        "\n",
        "# =======================\n",
        "# Step 1: Imports and Config\n",
        "# =======================\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "import zipfile\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "# Debug level: 0 = none, 1 = light, 2 = full\n",
        "DEBUG_LEVEL = 1\n",
        "\n",
        "# Partial run flag: True = first 10 rows only, False = full run\n",
        "PARTIAL_RUN = True\n",
        "\n",
        "# Force download: True = always download, False = skip if already exists\n",
        "FORCE_DOWNLOAD = False\n",
        "\n",
        "# Metadata only: True = only metadata, no downloads\n",
        "METADATA_ONLY = False\n",
        "\n",
        "# Configuration\n",
        "BASE_URL = \"https://www.wbdg.org\"\n",
        "URLS = {\n",
        "    \"active_page1\": \"https://www.wbdg.org/dod/ufc?field_status_value=1&field_series_value=All\",\n",
        "    \"active_page2\": \"https://www.wbdg.org/dod/ufc?field_status_value=1&field_series_value=All&page=1\",\n",
        "    \"inactive\": \"https://www.wbdg.org/dod/ufc?field_status_value=2&field_series_value=All\",\n",
        "    \"archived\": \"https://www.wbdg.org/dod/ufc?field_status_value=3&field_series_value=All\"\n",
        "}\n",
        "\n",
        "# Reference files\n",
        "related_files = [\n",
        "    (\"DoD Directive 4270.5\", \"https://www.esd.whs.mil/Portals/54/Documents/DD/issuances/dodd/427005p.pdf\"),\n",
        "    (\"UFC Implementation Guide\", \"https://www.wbdg.org/FFC/DOD/ufc_implementation.pdf\"),\n",
        "    (\"MIL-STD-3007\", \"https://www.wbdg.org/FFC/FEDMIL/milstd3007g.pdf\"),\n",
        "    (\"UFC Word Template (2025)\", \"https://www.wbdg.org/FFC/DOD/ufc_word_template_05_06_2025.docx\"),\n",
        "    (\"UFC UFGS Change Rev Policy\", \"https://www.wbdg.org/FFC/DOD/UFC/ufc_ufgs_chg_rev_policy.pdf\")\n",
        "]\n",
        "\n",
        "# UFC Complete 5-volume downloads\n",
        "ufc_complete_downloads = [\n",
        "    (\"UFC Complete Volume 1\", \"https://www.wbdg.org/FFC/DOD/UFC/UFC_Complete_1-200-01_thru_3-220-20.pdf\"),\n",
        "    (\"UFC Complete Volume 2\", \"https://www.wbdg.org/FFC/DOD/UFC/UFC_Complete_3-230-01_thru_3-340-02.pdf\"),\n",
        "    (\"UFC Complete Volume 3\", \"https://www.wbdg.org/FFC/DOD/UFC/UFC_Complete_3-400-02_thru_3-810-01N.pdf\"),\n",
        "    (\"UFC Complete Volume 4\", \"https://www.wbdg.org/FFC/DOD/UFC/UFC_Complete_4-010-01_thru_4-159-03.pdf\"),\n",
        "    (\"UFC Complete Volume 5\", \"https://www.wbdg.org/FFC/DOD/UFC/UFC_Complete_FC_4-171-06N_thru_4-860_03.pdf\")\n",
        "]\n",
        "\n",
        "# Utility: controlled print\n",
        "def debug_print(msg, level=1):\n",
        "    if DEBUG_LEVEL >= level:\n",
        "        print(msg)\n",
        "\n",
        "# Metadata extraction\n",
        "def get_metadata_from_detail_page(detail_url):\n",
        "    debug_print(f\"[DEBUG] Fetching metadata page: {detail_url}\", level=2)\n",
        "    resp = requests.get(detail_url)\n",
        "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "\n",
        "    # Extract series as separate lines, remove \"Series\" label\n",
        "    series_div = soup.find(\"div\", class_=\"field--name-field-series\")\n",
        "    series_list = None\n",
        "    if series_div:\n",
        "        series_items = series_div.find_all(string=True)\n",
        "        series_list = [item.strip() for item in series_items if item.strip() and item.strip() != \"Series\"]\n",
        "\n",
        "    download_link_tag = soup.find(\"a\", href=re.compile(r'\\.pdf$|\\.docx$|\\.zip$'))\n",
        "    download_link = BASE_URL + download_link_tag[\"href\"] if download_link_tag else None\n",
        "\n",
        "    # Clean summary\n",
        "    summary_div = soup.find(\"div\", class_=\"field--name-field-summary\")\n",
        "    summary = None\n",
        "    if summary_div:\n",
        "        summary_text = summary_div.get_text(separator=\" \").replace(\"Summary\", \"\").strip()\n",
        "        summary = re.sub(r\"\\s+\", \" \", summary_text)\n",
        "\n",
        "    metadata = {\n",
        "        \"status\": soup.find(\"div\", class_=\"field--name-field-status\").get_text(strip=True).replace(\"Status\", \"\").strip() if soup.find(\"div\", class_=\"field--name-field-status\") else None,\n",
        "        \"publish_date\": soup.find(\"div\", class_=\"field--name-field-publish-date\").get_text(strip=True).replace(\"Publish Date\", \"\").strip() if soup.find(\"div\", class_=\"field--name-field-publish-date\") else None,\n",
        "        \"archived / rescinded date\": soup.find(\"div\", class_=\"field--name-field-archived-date\").get_text(strip=True).replace(\"Archived / Rescinded Date\", \"\").strip() if soup.find(\"div\", class_=\"field--name-field-archived-date\") else None,\n",
        "        \"pages\": soup.find(\"div\", class_=\"field--name-field-pages\").get_text(strip=True).replace(\"Pages\", \"\").strip() if soup.find(\"div\", class_=\"field--name-field-pages\") else None,\n",
        "        \"series\": series_list,\n",
        "        \"download_link\": download_link,\n",
        "        \"summary\": summary\n",
        "    }\n",
        "\n",
        "    debug_print(f\"[DEBUG] Metadata parsed: {metadata}\", level=2)\n",
        "    return metadata\n",
        "\n",
        "def parse_title_fields(raw_title):\n",
        "    replaced_by = None\n",
        "    if \"Replaced by\" in raw_title:\n",
        "        parts = raw_title.split(\"Replaced by\")\n",
        "        main_part = parts[0].strip().rstrip(\",\")\n",
        "        replaced_by = \"Replaced by \" + parts[1].strip()\n",
        "    else:\n",
        "        main_part = raw_title\n",
        "\n",
        "    match = re.match(r\"^(UFC)\\s+([\\d\\-A-Z]+)\\s+(.*)\", main_part)\n",
        "    if match:\n",
        "        ufc_prefix, ufc_number, title = match.groups()\n",
        "    else:\n",
        "        ufc_prefix, ufc_number, title = None, None, main_part\n",
        "\n",
        "    return {\"ufc_prefix\": ufc_prefix, \"ufc_number\": ufc_number, \"title\": title, \"replaced_by\": replaced_by}\n",
        "\n",
        "def scrape_ufc_list(url, status):\n",
        "    debug_print(f\"[INFO] Scraping UFC list from: {url}\", level=1)\n",
        "    resp = requests.get(url)\n",
        "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "\n",
        "    rows = soup.find_all(\"tr\")\n",
        "    debug_print(f\"[DEBUG] Found {len(rows)} table rows.\", level=2)\n",
        "\n",
        "    row_limit = 10 if PARTIAL_RUN else len(rows)\n",
        "    ufc_entries = []\n",
        "\n",
        "    for i, row in enumerate(rows[:row_limit]):\n",
        "        cols = row.find_all(\"td\")\n",
        "        debug_print(f\"[DEBUG] Row {i} Columns:\", level=2)\n",
        "        for idx, col in enumerate(cols):\n",
        "            debug_print(f\"    Col {idx}: {col.get_text(strip=True)}\", level=2)\n",
        "\n",
        "        if len(cols) < 4:\n",
        "            debug_print(f\"[DEBUG] Row {i}: Skipped (not enough columns).\", level=2)\n",
        "            continue\n",
        "\n",
        "        raw_title = cols[0].get_text(strip=True)\n",
        "        parsed_title = parse_title_fields(raw_title)\n",
        "        detail_url = BASE_URL + cols[0].find(\"a\")[\"href\"]\n",
        "\n",
        "        metadata = get_metadata_from_detail_page(detail_url)\n",
        "\n",
        "        ufc_entry = {\n",
        "            \"ufc_full_name\": raw_title,\n",
        "            \"ufc_number\": parsed_title[\"ufc_number\"],\n",
        "            \"ufc_prefix\": parsed_title[\"ufc_prefix\"],\n",
        "            \"ufc_title\": parsed_title[\"title\"],\n",
        "            \"pages\": metadata[\"pages\"],\n",
        "            \"series\": metadata[\"series\"],\n",
        "            \"status\": metadata[\"status\"],\n",
        "            \"publish_date\": cols[1].get_text(strip=True) or None,\n",
        "            \"change_date\": cols[3].get_text(strip=True) or None,\n",
        "            \"archived / rescinded date\": metadata[\"archived / rescinded date\"],\n",
        "            \"replaced_by\": parsed_title[\"replaced_by\"],\n",
        "            \"download_link\": metadata[\"download_link\"],\n",
        "            \"metadata_link\": detail_url,\n",
        "            \"summary\": metadata[\"summary\"]\n",
        "        }\n",
        "        debug_print(f\"[DEBUG] Final UFC entry JSON:\\n{json.dumps(ufc_entry, indent=2)}\", level=2)\n",
        "        ufc_entries.append(ufc_entry)\n",
        "        time.sleep(1)\n",
        "\n",
        "    return ufc_entries\n",
        "\n",
        "# Scrape all UFCs\n",
        "all_ufcs = []\n",
        "for page_key in [\"active_page1\", \"active_page2\", \"inactive\", \"archived\"]:\n",
        "    all_ufcs += scrape_ufc_list(URLS[page_key], page_key)\n",
        "\n",
        "# Add UFC Complete entries\n",
        "for name, url in ufc_complete_downloads:\n",
        "    entry = {\n",
        "        \"ufc_full_name\": name,\n",
        "        \"ufc_number\": None,\n",
        "        \"ufc_prefix\": None,\n",
        "        \"ufc_title\": name,\n",
        "        \"pages\": None,\n",
        "        \"series\": None,\n",
        "        \"status\": \"Reference\",\n",
        "        \"publish_date\": \"06/02/2025\",\n",
        "        \"change_date\": None,\n",
        "        \"archived / rescinded date\": None,\n",
        "        \"replaced_by\": None,\n",
        "        \"download_link\": url,\n",
        "        \"metadata_link\": \"https://www.wbdg.org/dod/ufc/ufc-complete\",\n",
        "        \"summary\": \"Active UFCs combined into five PDF documents\"\n",
        "    }\n",
        "    all_ufcs.append(entry)\n",
        "\n",
        "# Save JSON\n",
        "debug_print(\"[INFO] Saving final JSON data...\", level=1)\n",
        "with open(\"wbdg_ufc_metadata_parsed_debug.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(all_ufcs, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "if not METADATA_ONLY:\n",
        "    download_dir = \"wbdg_ufc_downloads\"\n",
        "    os.makedirs(download_dir, exist_ok=True)\n",
        "\n",
        "    downloaded_files = []\n",
        "    total_files = sum(1 for entry in all_ufcs if entry.get(\"download_link\"))\n",
        "    file_counter = 1\n",
        "\n",
        "    for entry in all_ufcs:\n",
        "        download_url = entry.get(\"download_link\")\n",
        "        if download_url:\n",
        "            filename = os.path.basename(urlparse(download_url).path)\n",
        "            # Determine status\n",
        "            status = entry.get(\"status\")\n",
        "            if status is None:\n",
        "                raise ValueError(f\"Missing status for entry: {entry.get('ufc_full_name')}\")\n",
        "            status_lower = status.lower()\n",
        "            if status_lower in [\"active\", \"inactive\", \"archived\"]:\n",
        "                status_dir = os.path.join(download_dir, status.capitalize())\n",
        "            elif status_lower == \"reference\":\n",
        "                status_dir = os.path.join(download_dir, \"Reference\")\n",
        "            else:\n",
        "                raise ValueError(f\"Unexpected status '{status}' for entry: {entry.get('ufc_full_name')}\")\n",
        "            os.makedirs(status_dir, exist_ok=True)\n",
        "            filepath = os.path.join(status_dir, filename)\n",
        "\n",
        "            debug_print(f\"[DOWNLOAD {file_counter}/{total_files}] {filename} (Status: {status})\", level=1)\n",
        "            try:\n",
        "                if FORCE_DOWNLOAD or not os.path.exists(filepath):\n",
        "                    resp = requests.get(download_url)\n",
        "                    with open(filepath, \"wb\") as f:\n",
        "                        f.write(resp.content)\n",
        "                    downloaded_files.append(filepath)\n",
        "                    debug_print(f\"[DOWNLOAD {file_counter}/{total_files}] Downloaded: {filename}\", level=1)\n",
        "                else:\n",
        "                    debug_print(f\"[DOWNLOAD {file_counter}/{total_files}] Skipped (exists): {filename}\", level=1)\n",
        "            except Exception as e:\n",
        "                debug_print(f\"[DOWNLOAD {file_counter}/{total_files}] Failed for {filename}: {e}\", level=1)\n",
        "            file_counter += 1\n",
        "\n",
        "    # Create zip archive\n",
        "    zip_filename = \"wbdg_ufc_downloads.zip\"\n",
        "    with zipfile.ZipFile(zip_filename, \"w\") as zipf:\n",
        "        for file in downloaded_files:\n",
        "            arcname = os.path.relpath(file, start=download_dir)\n",
        "            zipf.write(file, arcname)\n",
        "            debug_print(f\"[ZIP] Added to zip: {arcname}\", level=1)\n",
        "\n",
        "    debug_print(\"[DONE] All UFC and reference data downloaded and zipped.\", level=1)\n",
        "else:\n",
        "    debug_print(\"[DONE] Metadata-only run complete!\", level=1)\n"
      ]
    }
  ]
}