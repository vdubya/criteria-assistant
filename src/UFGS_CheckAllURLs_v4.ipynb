{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vdubya/criteria-assistant/blob/main/src/UFGS_CheckAllURLs_v4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Script Logic for Google Colab - Concurrency Comparison Test\n",
        "\n",
        "# Pip Installs (uncomment and run this line in a cell if needed)\n",
        "!pip install reachable==0.7.0 httpx fake-useragent lxml pandas openpyxl\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "import re # For regular expressions\n",
        "import pandas as pd\n",
        "import httpx\n",
        "import time\n",
        "import shutil\n",
        "import sys\n",
        "import urllib.parse\n",
        "import asyncio\n",
        "import traceback # For detailed error printing\n",
        "\n",
        "# --- Reachable Library Imports ---\n",
        "async_components_imported_successfully = False\n",
        "is_reachable_async_func = None\n",
        "AsyncClient_class = None\n",
        "TaskPool_class = None\n",
        "\n",
        "try:\n",
        "    from reachable import is_reachable_async as imported_is_reachable_async\n",
        "    is_reachable_async_func = imported_is_reachable_async\n",
        "    from reachable.client import AsyncClient as imported_AsyncClient\n",
        "    AsyncClient_class = imported_AsyncClient\n",
        "    from reachable.pool import TaskPool as imported_TaskPool\n",
        "    TaskPool_class = imported_TaskPool\n",
        "    async_components_imported_successfully = True\n",
        "    print(\"Successfully imported async components from 'reachable'.\")\n",
        "except ImportError as e:\n",
        "    print(f\"CRITICAL ERROR: Could not import required async components from 'reachable': {e}\")\n",
        "    print(\"Please ensure 'reachable==0.7.0' (AlexMili/Reachable on PyPI) is installed correctly.\")\n",
        "    is_reachable_async_func = lambda *args, **kwargs: asyncio.sleep(0)\n",
        "    class AsyncClientPlaceholder:\n",
        "        async def __aenter__(self): await asyncio.sleep(0); return self\n",
        "        async def __aexit__(self, *args): await asyncio.sleep(0)\n",
        "    AsyncClient_class = AsyncClientPlaceholder\n",
        "    class TaskPoolPlaceholder:\n",
        "        def __init__(self, *args, **kwargs): self._results = []\n",
        "        async def put(self, coro): await asyncio.sleep(0); self._results.append(None)\n",
        "        async def join(self): await asyncio.sleep(0)\n",
        "    TaskPool_class = TaskPoolPlaceholder\n",
        "\n",
        "# Imports for Excel formatting\n",
        "from openpyxl.styles import Font, PatternFill, colors, Alignment, Border, Side, NumberFormatDescriptor\n",
        "from openpyxl.formatting.rule import FormulaRule\n",
        "from openpyxl.utils import get_column_letter\n",
        "from openpyxl.worksheet.worksheet import Worksheet\n",
        "from openpyxl.cell import MergedCell\n",
        "from openpyxl.worksheet.table import Table, TableStyleInfo\n",
        "\n",
        "# --- Configuration ---\n",
        "DO_FULL_PROCESSING = True\n",
        "debug = False # Set to True for verbose output, False for cleaner comparison output\n",
        "\n",
        "# --- Constants ---\n",
        "ufgs_download_url = \"https://www.wbdg.org/FFC/DOD/UFGS/UFGS_M.zip\"\n",
        "zip_file_path = '/content/UFGS.zip'\n",
        "extract_path = '/content/sec_files'\n",
        "output_excel_path_base = '/content/SEC_URL_references_with_certainty' # Base name for output files\n",
        "\n",
        "TARGET_SPECS_DATA = [\n",
        "    (\"02 84 16\", \"Electrical\"), (\"02 84 33\", \"Electrical\"), (\"08 34 49.00 20\", \"Electrical\"),\n",
        "    (\"08 71 63.10\", \"Electrical\"), (\"26 05 13.00 10\", \"Electrical\"), (\"26 05 19.00 10\", \"Electrical\"),\n",
        "    (\"26 05 33\", \"Electrical\"), (\"26 05 48\", \"Electrical\"), (\"26 05 73\", \"Electrical\"),\n",
        "    (\"26 08 00\", \"Electrical\"), (\"26 11 13.00 20\", \"Electrical\"), (\"26 11 14.00 10\", \"Electrical\"),\n",
        "    (\"26 11 16\", \"Electrical\"), (\"26 12 19\", \"Electrical\"), (\"26 12 21\", \"Electrical\"),\n",
        "    (\"26 13 00\", \"Electrical\"), (\"26 13 01\", \"Electrical\"), (\"26 13 02\", \"Electrical\"),\n",
        "    (\"26 13 13\", \"Electrical\"), (\"26 13 14\", \"Electrical\"), (\"26 13 32\", \"Electrical\"),\n",
        "    (\"26 19 13\", \"Electrical\"), (\"26 20 00\", \"Electrical\"), (\"26 22 00.00 10\", \"Electrical\"),\n",
        "    (\"26 23 00\", \"Electrical\"), (\"26 24 13\", \"Electrical\"), (\"26 27 29\", \"Electrical\"),\n",
        "    (\"26 28 00.00 10\", \"Electrical\"), (\"26 29 01.00 10\", \"Electrical\"), (\"26 29 02.00 10\", \"Electrical\"),\n",
        "    (\"26 29 23\", \"Electrical\"), (\"26 31 00\", \"Electrical\"), (\"26 32 15\", \"Electrical\"),\n",
        "    (\"26 33 00\", \"Electrical\"), (\"26 33 53\", \"Electrical\"), (\"26 35 43\", \"Electrical\"),\n",
        "    (\"26 35 44\", \"Electrical\"), (\"26 36 23\", \"Electrical\"), (\"26 41 00\", \"Electrical\"),\n",
        "    (\"26 42 13\", \"Electrical\"), (\"26 42 15\", \"Electrical\"), (\"26 42 17\", \"Electrical\"),\n",
        "    (\"26 42 19.00 10\", \"Electrical\"), (\"26 51 00\", \"Electrical\"), (\"26 55 53\", \"Electrical\"),\n",
        "    (\"26 56 00\", \"Electrical\"), (\"26 56 20\", \"Electrical\"), (\"27 05 13.43\", \"Electrical\"),\n",
        "    (\"27 05 26\", \"Electrical\"), (\"27 05 29.00 10\", \"Electrical\"), (\"27 10 00\", \"Electrical\"),\n",
        "    (\"27 41 00\", \"Electrical\"), (\"27 51 16\", \"Electrical\"), (\"27 51 23\", \"Electrical\"),\n",
        "    (\"27 53 19\", \"Electrical\"), (\"28 08 10\", \"Electrical\"), (\"28 10 05\", \"Electrical\"),\n",
        "    (\"28 20 02\", \"Electrical\"), (\"33 71 01\", \"Electrical\"), (\"33 71 02\", \"Electrical\"),\n",
        "    (\"33 82 00\", \"Electrical\"), (\"34 60 13\", \"Electrical\"), (\"35 20 20\", \"Electrical\"),\n",
        "    (\"48 14 00\", \"Electrical\"), (\"48 15 00\", \"Electrical\"), (\"48 16 00\", \"Electrical\")\n",
        "]\n",
        "\n",
        "url_pattern = re.compile(r'<URL(?:\\s+HREF=\"([^\"]*)\")?[^>]*>(.*?)</URL>', re.IGNORECASE | re.DOTALL)\n",
        "scn_pattern = re.compile(r'<SCN>(.*?)</SCN>', re.IGNORECASE | re.DOTALL)\n",
        "stl_pattern = re.compile(r'<STL>(.*?)</STL>', re.IGNORECASE | re.DOTALL)\n",
        "dte_pattern = re.compile(r'<DTE>(.*?)</DTE>', re.IGNORECASE | re.DOTALL)\n",
        "pra_pattern = re.compile(r'<PRA>(.*?)</PRA>', re.IGNORECASE | re.DOTALL)\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def debug_print(message):\n",
        "    if debug:\n",
        "        print(message)\n",
        "\n",
        "def is_email(url_to_check):\n",
        "    if not isinstance(url_to_check, str): return False\n",
        "    return url_to_check.startswith(\"mailto:\") or '@' in url_to_check\n",
        "\n",
        "def is_wbdg_url(url_to_check):\n",
        "    if isinstance(url_to_check, str): return \"wbdg.org\" in url_to_check.lower()\n",
        "    return False\n",
        "\n",
        "def parse_reachable_result(result_dict: dict, original_url_input: str, current_run_error_counts: dict):\n",
        "    validation_status = \"FAIL\"; current_status_code = None; current_final_url = original_url_input\n",
        "    error_message = \"Initial parsing error\"; check_certainty = \"HIGH\"\n",
        "\n",
        "    if not isinstance(result_dict, dict):\n",
        "        error_message = f\"Invalid result: expected dict, got {type(result_dict)}\"\n",
        "        if original_url_input not in current_run_error_counts: current_run_error_counts[original_url_input] = 0\n",
        "        current_run_error_counts[original_url_input] += 1\n",
        "        return (\"PROCESSING_ERROR\", None, original_url_input, error_message, is_wbdg_url(original_url_input), \"LOW\")\n",
        "\n",
        "    url_str_from_dict = result_dict.get('original_url', original_url_input)\n",
        "    wbdg_check = is_wbdg_url(url_str_from_dict)\n",
        "\n",
        "    if result_dict.get('is_email_flag_for_parser'):\n",
        "         return (\"EMAIL\", None, url_str_from_dict, None, wbdg_check, \"HIGH\")\n",
        "    if result_dict.get('is_invalid_flag_for_parser'):\n",
        "        return (\"INVALID\", None, url_str_from_dict, \"Invalid URL format (pre-checked)\", wbdg_check, \"HIGH\")\n",
        "    if result_dict.get('error_detail_worker') or result_dict.get('error_detail_setup'):\n",
        "        error_message = f\"Worker/Setup Error: {result_dict.get('error_name', 'Unknown')} - {result_dict.get('error_detail_worker') or result_dict.get('error_detail_setup')}\"\n",
        "        validation_status = result_dict.get('custom_status', \"FAIL_WORKER\")\n",
        "        current_final_url = url_str_from_dict\n",
        "        current_status_code = result_dict.get('status_code')\n",
        "        if original_url_input not in current_run_error_counts: current_run_error_counts[original_url_input] = 0\n",
        "        current_run_error_counts[original_url_input] += 1\n",
        "        return (validation_status, current_status_code, current_final_url, error_message, wbdg_check, \"HIGH\")\n",
        "\n",
        "    success = result_dict.get('success', False)\n",
        "    debug_print(f\"    Parsing result for '{url_str_from_dict}'. Library success: {success}, Error Name: {result_dict.get('error_name')}, Status Code: {result_dict.get('status_code')}\")\n",
        "    status_code_from_dict = result_dict.get('status_code')\n",
        "    error_name_from_dict = result_dict.get('error_name')\n",
        "    current_final_url = result_dict.get('final_url', url_str_from_dict)\n",
        "    current_status_code = status_code_from_dict if status_code_from_dict != -1 else None\n",
        "\n",
        "    response_text_content = None\n",
        "    httpx_response_obj = result_dict.get('response')\n",
        "    if httpx_response_obj and hasattr(httpx_response_obj, 'text'):\n",
        "        try:\n",
        "            response_text_content = httpx_response_obj.text\n",
        "            if response_text_content is None: response_text_content = \"\"\n",
        "        except Exception as e_text:\n",
        "            debug_print(f\"    Error accessing .text from response object for {url_str_from_dict}: {e_text}\")\n",
        "            response_text_content = \"\"\n",
        "        if debug and response_text_content:\n",
        "            debug_print(f\"    Response text for {url_str_from_dict} (first 100): '{response_text_content[:100]}'\")\n",
        "    elif debug and result_dict.get('include_response'):\n",
        "         debug_print(f\"    include_response=True but 'response' key was '{httpx_response_obj}' (type: {type(httpx_response_obj)}) and unusable for text for {url_str_from_dict}. Keys in dict: {list(result_dict.keys())}\")\n",
        "\n",
        "    is_parked = result_dict.get('is_parking_domain', False)\n",
        "    parked_domain_message = \"Domain may be parked.\" if is_parked else \"\"\n",
        "\n",
        "    if wbdg_check and response_text_content:\n",
        "        debug_print(f\"    WBDG check for {url_str_from_dict} with text content.\")\n",
        "        text_content_lower = response_text_content.lower()\n",
        "        wbdg_error_phrases = [\"oops\", \"an error has occurred\", \"page canâ€™t be found\", \"page not found\", \"error processing your request\", \"server error\", \"unable to process\", \"temporarily unavailable\"]\n",
        "        if any(phrase in text_content_lower for phrase in wbdg_error_phrases):\n",
        "            validation_status = \"WARN_WBDG_CONTENT_ERROR\"\n",
        "            error_message = \"WBDG: Content check triggered an error page.\"\n",
        "            if is_parked: error_message += f\" {parked_domain_message}\"\n",
        "            check_certainty = \"HIGH\"\n",
        "            if original_url_input not in current_run_error_counts: current_run_error_counts[original_url_input] = 0\n",
        "            current_run_error_counts[original_url_input] += 1\n",
        "            debug_print(f\"    ==> Final parse_reachable_result for {original_url_input}: Status={validation_status}, Code={current_status_code}, FinalURL='{current_final_url}', Msg='{error_message}', WBDG={wbdg_check}, Certainty={check_certainty}\")\n",
        "            return (validation_status, current_status_code, current_final_url, error_message, wbdg_check, check_certainty)\n",
        "\n",
        "    if success:\n",
        "        validation_status = \"PASS\"\n",
        "        error_message = parked_domain_message if is_parked else None\n",
        "        check_certainty = \"MEDIUM\" if is_parked and validation_status == \"PASS\" else \"HIGH\"\n",
        "    else:\n",
        "        validation_status = \"FAIL\"\n",
        "        inline_tag_detected_in_original = False\n",
        "        if re.search(r'<[^>]+>', original_url_input):\n",
        "            inline_tag_detected_in_original = True\n",
        "            debug_print(f\"    Inline tag detected in original URL: {original_url_input}\")\n",
        "\n",
        "        if error_name_from_dict == 'InvalidURL' and inline_tag_detected_in_original:\n",
        "            error_message = \"Failed. Library reported 'InvalidURL', likely due to inline tag(s) (e.g., <BRK/>) in the URL.\"\n",
        "        elif error_name_from_dict:\n",
        "            error_message = f\"Failed. Error: {error_name_from_dict}.\"\n",
        "            if inline_tag_detected_in_original:\n",
        "                error_message += \" Suspected inline tag(s) in URL may have contributed.\"\n",
        "        else:\n",
        "            error_message = \"Failed (success=False).\"\n",
        "            if inline_tag_detected_in_original:\n",
        "                error_message += \" Suspected inline tag(s) in URL.\"\n",
        "\n",
        "        if status_code_from_dict is not None:\n",
        "            error_message += f\" (Raw Status: {status_code_from_dict})\"\n",
        "\n",
        "        if is_parked:\n",
        "            parked_note = parked_domain_message\n",
        "            if error_message and not error_message.endswith('.'): error_message += \".\"\n",
        "            error_message = f\"{error_message} {parked_note}\" if error_message else parked_note\n",
        "\n",
        "        check_certainty = \"HIGH\"\n",
        "        if original_url_input not in current_run_error_counts: current_run_error_counts[original_url_input] = 0\n",
        "        current_run_error_counts[original_url_input] += 1\n",
        "\n",
        "    debug_print(f\"    ==> Final parse_reachable_result for {original_url_input}: Status={validation_status}, Code={current_status_code}, FinalURL='{current_final_url}', Msg='{error_message}', WBDG={wbdg_check}, Certainty={check_certainty}\")\n",
        "    return (validation_status, current_status_code, current_final_url, error_message, wbdg_check, check_certainty)\n",
        "\n",
        "async def async_url_worker(url: str, client: AsyncClient_class, worker_params: dict):\n",
        "    if not is_reachable_async_func or not callable(is_reachable_async_func):\n",
        "        return {'original_url': url, 'success': False, 'error_name': 'LibFuncMissingAsync'}\n",
        "    try:\n",
        "        # Removed 'attempts' from direct pass as it caused TypeError.\n",
        "        # Relying on library's default for attempts.\n",
        "        params_for_call = {k: v for k, v in worker_params.items() if k != 'attempts'}\n",
        "        return await is_reachable_async_func(url, client=client, **params_for_call)\n",
        "    except TypeError as te:\n",
        "        debug_print(f\"  TypeError in async_url_worker for {url} calling is_reachable_async: {te}\")\n",
        "        return {'original_url': url, 'success': False, 'status_code': None, 'error_name': 'TypeErrorKwargs', 'error_detail_worker': str(te)}\n",
        "    except Exception as e:\n",
        "        debug_print(f\"  Exception in async_url_worker for {url}: {type(e).__name__} - {str(e)}\")\n",
        "        return {'original_url': url, 'success': False, 'status_code': None, 'error_name': type(e).__name__, 'error_detail_worker': str(e)}\n",
        "\n",
        "async def run_async_validators(urls_to_check: list, pool_size: int = 50):\n",
        "    results = []\n",
        "    if not async_components_imported_successfully or not AsyncClient_class or not TaskPool_class or AsyncClient_class.__name__ == 'AsyncClientPlaceholder' or TaskPool_class.__name__ == 'TaskPoolPlaceholder':\n",
        "        print(\"ERROR: Async components not properly available. Cannot run async validation.\")\n",
        "        for url in urls_to_check: results.append({'original_url': url, 'success': False, 'error_name': 'AsyncSetupErrorMain'})\n",
        "        return results\n",
        "\n",
        "    debug_print(f\"  run_async_validators: Initializing AsyncClient with library defaults (no explicit timeout argument).\")\n",
        "    async with AsyncClient_class() as client: # NO TIMEOUT ARGUMENT HERE\n",
        "        tasks_pool = TaskPool_class(workers=pool_size)\n",
        "        debug_print(f\"  Submitting {len(urls_to_check)} URLs to TaskPool with {pool_size} workers...\")\n",
        "\n",
        "        worker_call_params = {\n",
        "            \"headers\": None,\n",
        "            \"include_response\": True,\n",
        "            \"check_parking_domain\": True\n",
        "            # \"attempts\" is removed as it caused TypeError with is_reachable_async for this library version\n",
        "        }\n",
        "        for url_item in urls_to_check: await tasks_pool.put(async_url_worker(url_item, client, worker_call_params))\n",
        "\n",
        "        debug_print(f\"  All tasks submitted. Awaiting TaskPool.join()...\")\n",
        "        await tasks_pool.join()\n",
        "        debug_print(f\"  TaskPool.join() completed.\")\n",
        "\n",
        "        if hasattr(tasks_pool, '_results'): results = tasks_pool._results\n",
        "        elif hasattr(tasks_pool, 'results'): results = tasks_pool.results\n",
        "        else:\n",
        "            print(\"ERROR: Could not retrieve results from TaskPool.\")\n",
        "            results = [{'original_url': u, 'success': False, 'error_name': 'TaskPoolResultErrorMain'} for u in urls_to_check]\n",
        "        debug_print(f\"  Retrieved {len(results)} results from TaskPool.\")\n",
        "    return results\n",
        "\n",
        "def parse_sec_file_for_urls(file_path, current_id_counter):\n",
        "    debug_print(f\"  Parsing for URLs: {os.path.basename(file_path)}\")\n",
        "    extracted_rows = []\n",
        "    urls_found_in_file = 0\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as file: content = file.read()\n",
        "        for match in url_pattern.finditer(content):\n",
        "            url_href, url_txt = (match.group(1) if match.group(1) else None), (match.group(2).strip() if match.group(2) else '')\n",
        "            primary_url = None\n",
        "            if url_href: primary_url = url_href.strip()\n",
        "            elif url_txt and '.' in url_txt:\n",
        "                cleaned_url_txt = re.sub(r'<[^>]+>', '', url_txt.strip()).strip()\n",
        "                if ' ' not in cleaned_url_txt and (cleaned_url_txt.count('.') >= 1 or '@' in cleaned_url_txt or cleaned_url_txt.lower().startswith(\"www.\")): primary_url = cleaned_url_txt\n",
        "                elif cleaned_url_txt.lower().startswith(\"http\"): primary_url = cleaned_url_txt\n",
        "            if not primary_url: continue\n",
        "            char_offset, line_number = match.start(), content.count('\\n', 0, match.start()) + 1\n",
        "            extracted_rows.append((current_id_counter, os.path.basename(file_path), line_number, char_offset, url_txt, url_href, primary_url))\n",
        "            current_id_counter += 1\n",
        "            urls_found_in_file += 1\n",
        "    except Exception as e: debug_print(f\"  Error parsing URLs in {os.path.basename(file_path)}: {e}\")\n",
        "    debug_print(f\"  Finished parsing URLs for {os.path.basename(file_path)}. Found {urls_found_in_file} URLs.\")\n",
        "    return extracted_rows, current_id_counter\n",
        "\n",
        "def extract_file_metadata(file_path):\n",
        "    filename = os.path.basename(file_path)\n",
        "    metadata = {'Filename': filename, 'Section Number': \"Not Found\", 'Section Title': \"Not Found\", 'Date': \"Not Found\", 'Preparing Authority': \"Not Found\" }\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as file: content_start = file.read(8192)\n",
        "        if (m := scn_pattern.search(content_start)): metadata['Section Number'] = m.group(1).strip()\n",
        "        if (m := stl_pattern.search(content_start)): metadata['Section Title'] = ' '.join(m.group(1).split()).strip()\n",
        "        if (m := dte_pattern.search(content_start)): metadata['Date'] = m.group(1).strip()\n",
        "        if (m := pra_pattern.search(content_start)): metadata['Preparing Authority'] = ' '.join(m.group(1).split()).strip()\n",
        "    except Exception as e: debug_print(f\"  Error extracting metadata from {filename}: {e}\")\n",
        "    return metadata\n",
        "\n",
        "def auto_fit_columns(worksheet: Worksheet, columns_to_fit=None, max_width=100, padding=3):\n",
        "    col_indices = [ord(c.upper()) - ord('A') + 1 for c in columns_to_fit] if columns_to_fit else range(1, worksheet.max_column + 1)\n",
        "    for col_idx in col_indices:\n",
        "        max_length = 0\n",
        "        column_letter = get_column_letter(col_idx)\n",
        "        for row_idx in range(1, worksheet.max_row + 1):\n",
        "            cell = worksheet.cell(row=row_idx, column=col_idx)\n",
        "            is_merged, cell_val_len = False, 0\n",
        "            for merged_range_obj in worksheet.merged_cells.ranges:\n",
        "                if cell.coordinate in merged_range_obj:\n",
        "                    if cell.row == merged_range_obj.min_row and cell.column == merged_range_obj.min_col:\n",
        "                        cell_val_len = len(str(worksheet.cell(row=merged_range_obj.min_row, column=merged_range_obj.min_col).value or \"\"))\n",
        "                    is_merged = True; break\n",
        "            if not is_merged and cell.value is not None: cell_val_len = len(str(cell.value or \"\"))\n",
        "            if cell_val_len > max_length: max_length = cell_val_len\n",
        "        worksheet.column_dimensions[column_letter].width = min(max(max_length + padding, 8), max_width)\n",
        "    debug_print(f\"Auto-fitted columns ({', '.join(columns_to_fit) if columns_to_fit else 'All'}) for sheet: {worksheet.title}\")\n",
        "\n",
        "async def async_process_all_ufgs_data(run_label:str = \"default_run\", concurrency_level_override=None, save_excel=True):\n",
        "    # Use run-specific variables for counters and maps\n",
        "    current_run_error_counts = {}\n",
        "    current_run_global_id_counter = 1\n",
        "    current_run_df_urls = pd.DataFrame()\n",
        "    current_run_validation_results_map = {}\n",
        "\n",
        "    script_start_time = time.time()\n",
        "    print(f\"--- [{run_label}] Main Async Processing Started at {time.ctime(script_start_time)} (Debug: {debug}) ---\")\n",
        "    try:\n",
        "        import reachable as r_check_main\n",
        "        print(f\"Reachable library version (main script): {getattr(r_check_main, '__version__', '0.7.0 (User Confirmed)')}\")\n",
        "    except ImportError: print(\"Could not import 'reachable' for version check in main script.\")\n",
        "    print(f\"httpx library version: {getattr(httpx, '__version__', 'N/A')}\")\n",
        "\n",
        "    if run_label == \"1_worker\" or (run_label != \"100_workers_use_cache\" and not os.path.exists(zip_file_path)):\n",
        "        if not ufgs_download_url or \"REPLACE_WITH_CORRECT_UFGS_ZIP_DOWNLOAD_LINK\" in ufgs_download_url:\n",
        "            print(f\"[{run_label}] Error: UFGS download URL not set. Exiting.\"); return pd.DataFrame()\n",
        "        debug_print(f\"[{run_label}] Downloading from: {ufgs_download_url}\")\n",
        "        try:\n",
        "            with httpx.Client(verify=False) as client:\n",
        "                with client.stream(\"GET\", ufgs_download_url, follow_redirects=True, timeout=60.0) as response:\n",
        "                    response.raise_for_status()\n",
        "                    with open(zip_file_path, 'wb') as f: [f.write(chunk) for chunk in response.iter_bytes(8192)]\n",
        "            print(f\"[{run_label}] Download complete: {zip_file_path}\")\n",
        "        except Exception as e: print(f\"[{run_label}] Download error: {e}\"); return pd.DataFrame()\n",
        "    elif os.path.exists(zip_file_path):\n",
        "         print(f\"[{run_label}] Using existing/cached zip file: {zip_file_path}\")\n",
        "    else:\n",
        "        print(f\"[{run_label}] Zip file logic error. Exiting.\"); return pd.DataFrame()\n",
        "\n",
        "    if not os.path.exists(zip_file_path): print(f\"[{run_label}] Zip file not found: {zip_file_path}\"); return pd.DataFrame()\n",
        "    if os.path.exists(extract_path): shutil.rmtree(extract_path)\n",
        "    os.makedirs(extract_path)\n",
        "    debug_print(f\"[{run_label}] Extracting zip file...\")\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref: zip_ref.extractall(extract_path)\n",
        "        debug_print(f\"[{run_label}] Extraction complete.\")\n",
        "    except Exception as e: print(f\"[{run_label}] Zip extraction error: {e}\"); return pd.DataFrame()\n",
        "\n",
        "    all_url_rows_list = []\n",
        "    sec_files_found_list = [os.path.join(r, i) for r, _, f in os.walk(extract_path) for i in f if i.lower().endswith('.sec')]\n",
        "    if not sec_files_found_list: print(f\"[{run_label}] No '.SEC' files found in {extract_path}.\"); return pd.DataFrame()\n",
        "    print(f\"[{run_label}] Found {len(sec_files_found_list)} .SEC files to process.\")\n",
        "    sec_files_found_list.sort()\n",
        "\n",
        "    file_metadata_list_collector = []\n",
        "    for file_path in sec_files_found_list:\n",
        "        debug_print(f\"  --- [{run_label}] Parsing file: {os.path.basename(file_path)} ---\")\n",
        "        file_metadata_list_collector.append(extract_file_metadata(file_path))\n",
        "        url_rows_temp, current_run_global_id_counter = parse_sec_file_for_urls(file_path, current_run_global_id_counter)\n",
        "        all_url_rows_list.extend(url_rows_temp)\n",
        "\n",
        "    df_file_metadata = pd.DataFrame(file_metadata_list_collector)\n",
        "    initial_url_cols = ['ID', 'SEC_FILE_NAME', 'DOC_LINE_NUMBER', 'DOC_CHAR_OFFSET', 'URL_TXT', 'URL_HREF', 'PRIMARY_URL']\n",
        "    if not all_url_rows_list: print(f\"[{run_label}] Warning: No URLs found.\")\n",
        "    current_run_df_urls = pd.DataFrame(all_url_rows_list, columns=initial_url_cols) if all_url_rows_list else pd.DataFrame(columns=initial_url_cols)\n",
        "\n",
        "    df_specs = pd.DataFrame(TARGET_SPECS_DATA, columns=['UFGS', 'Discipline_Spec'])\n",
        "    current_run_df_urls['Base_Spec'] = current_run_df_urls['SEC_FILE_NAME'].apply(lambda fn: fn[:-4] if isinstance(fn, str) and fn.lower().endswith('.sec') else fn)\n",
        "    current_run_df_urls = pd.merge(current_run_df_urls, df_specs, left_on='Base_Spec', right_on='UFGS', how='left')\n",
        "    current_run_df_urls.rename(columns={'Discipline_Spec': 'Discipline'}, inplace=True)\n",
        "    current_run_df_urls['Discipline'] = current_run_df_urls['Discipline'].fillna('Other')\n",
        "\n",
        "    def assign_division_01(row):\n",
        "        sec_file_name_str = str(row.get('SEC_FILE_NAME', ''))\n",
        "        if re.match(r\"^01(\\s|[^a-zA-Z0-9_.-]|$)\", sec_file_name_str): return \"DIVISION 01\"\n",
        "        return row['Discipline']\n",
        "\n",
        "    if 'SEC_FILE_NAME' in current_run_df_urls.columns and 'Discipline' in current_run_df_urls.columns:\n",
        "        current_run_df_urls['Discipline'] = current_run_df_urls.apply(assign_division_01, axis=1)\n",
        "\n",
        "    current_run_df_urls.drop(columns=['Base_Spec', 'UFGS'], inplace=True)\n",
        "\n",
        "    unique_primary_urls_raw = current_run_df_urls['PRIMARY_URL'].dropna().unique().tolist()\n",
        "    email_urls_for_map_collector = []\n",
        "    unique_primary_urls_for_batch_collector = []\n",
        "    invalid_urls_for_map_collector = {}\n",
        "\n",
        "    for u_url_str_raw in unique_primary_urls_raw:\n",
        "        u_url_str = str(u_url_str_raw).strip()\n",
        "        if not u_url_str: continue\n",
        "        if is_email(u_url_str): email_urls_for_map_collector.append(u_url_str)\n",
        "        else:\n",
        "            sanitized_check = re.sub(r'[^\\x20-\\x7E]+', '', u_url_str).strip()\n",
        "            sanitized_check = re.sub(r'<[^>]+>', '', sanitized_check).strip()\n",
        "            if not sanitized_check:\n",
        "                invalid_urls_for_map_collector[u_url_str] = parse_reachable_result({'original_url': u_url_str, 'is_invalid_flag_for_parser': True}, u_url_str, current_run_error_counts)\n",
        "            else: unique_primary_urls_for_batch_collector.append(u_url_str)\n",
        "\n",
        "    for email_url in email_urls_for_map_collector:\n",
        "        current_run_validation_results_map[email_url] = parse_reachable_result({'original_url': email_url, 'is_email_flag_for_parser': True}, email_url, current_run_error_counts)\n",
        "    for invalid_url, invalid_result_tuple in invalid_urls_for_map_collector.items():\n",
        "        current_run_validation_results_map[invalid_url] = invalid_result_tuple\n",
        "\n",
        "    print(f\"[{run_label}] Prepared {len(unique_primary_urls_for_batch_collector)} URLs for async batch validation.\")\n",
        "    url_val_start_time = time.time()\n",
        "\n",
        "    batch_results_list_data = []\n",
        "    if unique_primary_urls_for_batch_collector:\n",
        "        CONCURRENCY = concurrency_level_override if concurrency_level_override is not None \\\n",
        "                      else (100 if len(unique_primary_urls_for_batch_collector) > 500 else 50)\n",
        "        print(f\"[{run_label}] Starting async validation (Concurrency: {CONCURRENCY}, using library default timeouts)...\")\n",
        "        batch_results_list_data = await run_async_validators(unique_primary_urls_for_batch_collector, pool_size=CONCURRENCY)\n",
        "        print(f\"[{run_label}] Async URL Validation finished in {time.time() - url_val_start_time:.2f}s.\")\n",
        "\n",
        "        if isinstance(batch_results_list_data, list):\n",
        "            print(f\"[{run_label}] Processing {len(batch_results_list_data)} async results...\")\n",
        "            results_by_url_map = {res.get('original_url'): res for res in batch_results_list_data if isinstance(res, dict) and 'original_url' in res}\n",
        "            for url_k in unique_primary_urls_for_batch_collector:\n",
        "                res_d = results_by_url_map.get(url_k)\n",
        "                if res_d: current_run_validation_results_map[url_k] = parse_reachable_result(res_d, url_k, current_run_error_counts)\n",
        "                else:\n",
        "                    current_run_validation_results_map[url_k] = (\"BATCH_MISSING_RESULT\", None, url_k, \"Result missing/unmappable\", is_wbdg_url(url_k), \"LOW\")\n",
        "                    if url_k not in current_run_error_counts: current_run_error_counts[url_k] = 0; current_run_error_counts[url_k]+=1;\n",
        "        else:\n",
        "            debug_print(f\"[{run_label}] Async validation unexpected result type: {type(batch_results_list_data)}.\")\n",
        "            for url_v in unique_primary_urls_for_batch_collector:\n",
        "                current_run_validation_results_map[url_v] = (\"ASYNC_ERROR\", None, url_v, f\"Async bad type: {type(batch_results_list_data)}\", is_wbdg_url(url_v), \"LOW\")\n",
        "                if url_v not in current_run_error_counts: current_run_error_counts[url_v] = 0; current_run_error_counts[url_v]+=1;\n",
        "    else: print(f\"[{run_label}] No non-email, valid URLs for async batch.\")\n",
        "\n",
        "    result_cols = ['STATUS', 'RESPONSE_CODE', 'FINAL_URL', 'ERROR_MSG', 'IS_WBDG', 'CHECK_CERTAINTY']\n",
        "    if not current_run_df_urls.empty:\n",
        "        current_run_df_urls['PRIMARY_URL_STR'] = current_run_df_urls['PRIMARY_URL'].astype(str).str.strip()\n",
        "        val_series = current_run_df_urls['PRIMARY_URL_STR'].map(current_run_validation_results_map)\n",
        "        def get_res_part(item, idx):\n",
        "            if pd.isna(item) or not isinstance(item, tuple) or len(item) != 6:\n",
        "                col_nm = result_cols[idx] if idx < len(result_cols) else \"UNKNOWN_COLUMN\"\n",
        "                if col_nm == 'IS_WBDG': return False\n",
        "                if col_nm == 'CHECK_CERTAINTY': return \"UNKNOWN\"\n",
        "                if col_nm == 'STATUS': return \"NOT_MAPPED\"\n",
        "                return None\n",
        "            return item[idx]\n",
        "        for i, col_nm in enumerate(result_cols): current_run_df_urls[col_nm] = val_series.apply(lambda x: get_res_part(x, i))\n",
        "\n",
        "        for col, def_val, dtype in [('IS_WBDG', False, bool), ('CHECK_CERTAINTY', \"UNKNOWN\", str), ('STATUS', \"NOT_MAPPED\", str)]:\n",
        "            if col in current_run_df_urls.columns: current_run_df_urls[col] = current_run_df_urls[col].fillna(def_val).astype(dtype)\n",
        "            else: current_run_df_urls[col] = def_val\n",
        "\n",
        "        if 'PRIMARY_URL_STR' in current_run_df_urls.columns and not current_run_df_urls.empty :\n",
        "            if not current_run_df_urls['PRIMARY_URL_STR'].empty:\n",
        "                current_run_df_urls['COUNT'] = current_run_df_urls.groupby('PRIMARY_URL_STR')['PRIMARY_URL_STR'].transform('count').fillna(1).astype(int)\n",
        "            else: current_run_df_urls['COUNT'] = 1\n",
        "            current_run_df_urls.drop(columns=['PRIMARY_URL_STR'], inplace=True, errors='ignore')\n",
        "        else: current_run_df_urls['COUNT'] = 1\n",
        "    else:\n",
        "        excel_output_cols_list = initial_url_cols + result_cols + ['Discipline', 'COUNT']\n",
        "        current_run_df_urls = pd.DataFrame(columns=excel_output_cols_list)\n",
        "\n",
        "    if save_excel:\n",
        "        run_specific_excel_path = f\"{output_excel_path_base}_{run_label}.xlsx\"\n",
        "        debug_print(f\"[{run_label}] Saving DataFrame to Excel: {run_specific_excel_path}\")\n",
        "        try:\n",
        "            with pd.ExcelWriter(run_specific_excel_path, engine='openpyxl') as writer:\n",
        "                summary_sheet_name = 'Summary'; main_sheet_name = 'URL_Checks'; file_list_sheet_name = 'File_List'; spec_list_sheet_name = 'Spec_List'\n",
        "                # New column order for Excel output\n",
        "                output_columns_main = ['ID', 'SEC_FILE_NAME', 'Discipline', 'STATUS', 'COUNT',\n",
        "                                       'DOC_LINE_NUMBER', 'DOC_CHAR_OFFSET', 'IS_WBDG',\n",
        "                                       'CHECK_CERTAINTY', 'PRIMARY_URL', 'RESPONSE_CODE',\n",
        "                                       'URL_TXT', 'FINAL_URL', 'ERROR_MSG']\n",
        "                output_columns_filelist = ['Filename', 'Section Number', 'Section Title', 'Date', 'Preparing Authority']\n",
        "                ws_summary = writer.book.create_sheet(summary_sheet_name, 0)\n",
        "                header_font = Font(bold=False, size=24); sub_header_font = Font(bold=True); percent_format_string = '0.0%'; header_fill = PatternFill(start_color=\"DDEEFF\", end_color=\"DDEEFF\", fill_type=\"solid\"); grey_fill_summary_label = PatternFill(start_color=\"F0F0F0\", end_color=\"F0F0F0\", fill_type=\"solid\"); thin_border = Border(left=Side(style='thin'), right=Side(style='thin'), top=Side(style='thin'), bottom=Side(style='thin')); center_align = Alignment(horizontal='center', vertical='center'); left_align = Alignment(horizontal='left', vertical='center')\n",
        "                current_row = 1; overall_header_cell = ws_summary.cell(row=current_row, column=1, value=f\"Overall Summary ({run_label})\"); overall_header_cell.font = header_font; overall_header_cell.fill = header_fill; ws_summary.merge_cells(start_row=current_row, start_column=1, end_row=current_row, end_column=2); current_row += 1; overall_cells = {}; overall_start_row = current_row\n",
        "\n",
        "                current_summary_data_for_excel = {}\n",
        "                if not current_run_df_urls.empty and 'PRIMARY_URL' in current_run_df_urls.columns:\n",
        "                    total_urls_in_df = len(current_run_df_urls); unique_urls_in_df = current_run_df_urls['PRIMARY_URL'].nunique()\n",
        "                    fail_status_df = current_run_df_urls[current_run_df_urls['STATUS'] == 'FAIL']; total_fail_status = len(fail_status_df); unique_fail_status = fail_status_df['PRIMARY_URL'].nunique()\n",
        "                    wbdg_content_error_df = current_run_df_urls[current_run_df_urls['STATUS'] == 'WARN_WBDG_CONTENT_ERROR']; total_wbdg_content_errors = len(wbdg_content_error_df); unique_wbdg_content_errors = wbdg_content_error_df['PRIMARY_URL'].nunique()\n",
        "                    total_error_counted_occurrences = sum(current_run_error_counts.values()); unique_error_counted_urls = len(current_run_error_counts)\n",
        "                    current_summary_data_for_excel['Overall'] = {'Total URL Entries Processed (in DataFrame)': total_urls_in_df, 'Unique Primary URLs Encountered': unique_urls_in_df,'Total URL Failures (Status: FAIL)': total_fail_status, 'Total WBDG Content Warnings (Status: WARN_WBDG_CONTENT_ERROR)': total_wbdg_content_errors,'Total Error Occurrences (from error_counts dict)': total_error_counted_occurrences,'Unique URL Failures (Status: FAIL)': unique_fail_status, 'Unique WBDG Content Warnings': unique_wbdg_content_errors,'Unique URLs with Errors (from error_counts dict)': unique_error_counted_urls,'% Total Entries with Failures (Status: FAIL)': None, '% Unique URLs with Failures (Status: FAIL)': None,'% Total Entries with WBDG Warnings': None, '% Unique URLs with WBDG Warnings': None}\n",
        "                    discipline_summary_excel = {}\n",
        "                    if 'Discipline' in current_run_df_urls.columns:\n",
        "                        grouped = current_run_df_urls.groupby('Discipline')\n",
        "                        for name, group in grouped: disc_total_urls = len(group); disc_unique_urls = group['PRIMARY_URL'].nunique(); disc_fail_status_df = group[group['STATUS'] == 'FAIL']; disc_total_fail_status = len(disc_fail_status_df); disc_unique_fail_status = disc_fail_status_df['PRIMARY_URL'].nunique(); disc_wbdg_warn_df = group[group['STATUS'] == 'WARN_WBDG_CONTENT_ERROR']; disc_total_wbdg_warns = len(disc_wbdg_warn_df); disc_unique_wbdg_warns = disc_wbdg_warn_df['PRIMARY_URL'].nunique(); discipline_summary_excel[name] = {'Total URLs': disc_total_urls, 'Total Failures (FAIL)': disc_total_fail_status, 'Total WBDG Content Warnings': disc_total_wbdg_warns, 'Unique URLs': disc_unique_urls, 'Unique Failures (FAIL)': disc_unique_fail_status, 'Unique WBDG Content Warnings': disc_unique_wbdg_warns, '% Total URLs Failure (FAIL)': None, '% Unique URLs Failure (FAIL)': None, '% Total URLs WBDG Warning': None, '% Unique URLs WBDG Warning': None}\n",
        "                    discipline_order_map = {\"DIVISION 01\": 0, \"Electrical\": 1}; current_summary_data_for_excel['By Discipline'] = dict(sorted(discipline_summary_excel.items(), key=lambda item: (item[0] == 'Other', discipline_order_map.get(item[0], 2), item[0])))\n",
        "                else: current_summary_data_for_excel['Overall'] = {'Total URL Entries Processed (in DataFrame)': 0, 'Unique Primary URLs Encountered': 0,'Total URL Failures (Status: FAIL)': 0, 'Total WBDG Content Warnings (Status: WARN_WBDG_CONTENT_ERROR)':0,'Total Error Occurrences (from error_counts dict)': 0, 'Unique URL Failures (Status: FAIL)': 0, 'Unique WBDG Content Warnings':0, 'Unique URLs with Errors (from error_counts dict)':0,'% Total Entries with Failures (Status: FAIL)': 0.0, '% Unique URLs with Failures (Status: FAIL)': 0.0,'% Total Entries with WBDG Warnings': 0.0, '% Unique URLs with WBDG Warnings': 0.0}; current_summary_data_for_excel['By Discipline'] = {}\n",
        "\n",
        "                overall_keys_ordered = ['Total URL Entries Processed (in DataFrame)', 'Unique Primary URLs Encountered', 'Total URL Failures (Status: FAIL)', 'Total WBDG Content Warnings (Status: WARN_WBDG_CONTENT_ERROR)', 'Total Error Occurrences (from error_counts dict)', 'Unique URL Failures (Status: FAIL)', 'Unique WBDG Content Warnings', 'Unique URLs with Errors (from error_counts dict)', '% Total Entries with Failures (Status: FAIL)', '% Unique URLs with Failures (Status: FAIL)', '% Total Entries with WBDG Warnings', '% Unique URLs with WBDG Warnings']\n",
        "                for key in overall_keys_ordered: value = current_summary_data_for_excel['Overall'].get(key, 0 if 'URL' in key or 'Failures' in key or 'Warnings' in key or 'Error' in key else (0.0 if '%' in key else \"N/A\")); label_cell = ws_summary.cell(row=current_row, column=1, value=key); label_cell.alignment = left_align; label_cell.fill = grey_fill_summary_label; value_cell = ws_summary.cell(row=current_row, column=2, value=value); overall_cells[key] = value_cell.coordinate; value_cell.alignment = center_align; current_row += 1\n",
        "                def set_percentage_formula(ws, tc, nc, dc, dv): cell = ws[tc]; cell.value = f'=IFERROR({nc}/{dc},0)' if dv > 0 else 0.0; cell.number_format = percent_format_string; cell.alignment = center_align\n",
        "                set_percentage_formula(ws_summary, overall_cells['% Total Entries with Failures (Status: FAIL)'], overall_cells[\"Total URL Failures (Status: FAIL)\"], overall_cells[\"Total URL Entries Processed (in DataFrame)\"], current_summary_data_for_excel['Overall'].get('Total URL Entries Processed (in DataFrame)', 0))\n",
        "                set_percentage_formula(ws_summary, overall_cells['% Unique URLs with Failures (Status: FAIL)'], overall_cells[\"Unique URL Failures (Status: FAIL)\"], overall_cells[\"Unique Primary URLs Encountered\"], current_summary_data_for_excel['Overall'].get('Unique Primary URLs Encountered', 0))\n",
        "                set_percentage_formula(ws_summary, overall_cells['% Total Entries with WBDG Warnings'], overall_cells[\"Total WBDG Content Warnings (Status: WARN_WBDG_CONTENT_ERROR)\"], overall_cells[\"Total URL Entries Processed (in DataFrame)\"], current_summary_data_for_excel['Overall'].get('Total URL Entries Processed (in DataFrame)', 0))\n",
        "                set_percentage_formula(ws_summary, overall_cells['% Unique URLs with WBDG Warnings'], overall_cells[\"Unique WBDG Content Warnings\"], overall_cells[\"Unique Primary URLs Encountered\"], current_summary_data_for_excel['Overall'].get('Unique Primary URLs Encountered', 0))\n",
        "                overall_end_row = current_row - 1; current_row += 1\n",
        "                discipline_header_cell = ws_summary.cell(row=current_row, column=1, value=\"Summary by Discipline\"); discipline_header_cell.font = header_font; discipline_header_cell.fill = header_fill; ws_summary.merge_cells(start_row=current_row, start_column=1, end_row=current_row, end_column=11); [ws_summary.cell(row=current_row, column=c).fill for c in range(1,12)]; current_row += 1; discipline_start_row = current_row\n",
        "                if current_summary_data_for_excel['By Discipline']:\n",
        "                    disc_headers_ordered = ['Total URLs', 'Total Failures (FAIL)', 'Total WBDG Content Warnings', 'Unique URLs', 'Unique Failures (FAIL)', 'Unique WBDG Content Warnings', '% Total URLs Failure (FAIL)', '% Unique URLs Failure (FAIL)', '% Total URLs WBDG Warning', '% Unique URLs WBDG Warning']\n",
        "                    ws_summary.cell(row=current_row, column=1, value=\"Discipline\").font = sub_header_font; ws_summary.cell(row=current_row, column=1).fill = grey_fill_summary_label; ws_summary.cell(row=current_row, column=1).border = thin_border; ws_summary.cell(row=current_row, column=1).alignment = left_align\n",
        "                    for col_idx, header in enumerate(disc_headers_ordered): cell = ws_summary.cell(row=current_row, column=col_idx + 2, value=header); cell.font = sub_header_font; cell.fill = grey_fill_summary_label; cell.border = thin_border; cell.alignment = center_align\n",
        "                    current_row += 1\n",
        "                    for discipline, stats in current_summary_data_for_excel['By Discipline'].items():\n",
        "                        ws_summary.cell(row=current_row, column=1, value=discipline).font = sub_header_font; ws_summary.cell(row=current_row, column=1).border = thin_border; ws_summary.cell(row=current_row, column=1).alignment = left_align; discipline_cells = {}; col_idx = 2\n",
        "                        for key in disc_headers_ordered: value = stats.get(key, 0 if 'URL' in key or 'Failures' in key or 'Warning' in key else (0.0 if '%' in key else \"N/A\")); cell = ws_summary.cell(row=current_row, column=col_idx, value=value); discipline_cells[key] = cell.coordinate; cell.border = thin_border; cell.alignment = center_align; col_idx += 1\n",
        "                        set_percentage_formula(ws_summary, discipline_cells['% Total URLs Failure (FAIL)'], discipline_cells[\"Total Failures (FAIL)\"], discipline_cells[\"Total URLs\"], stats.get('Total URLs',0))\n",
        "                        set_percentage_formula(ws_summary, discipline_cells['% Unique URLs Failure (FAIL)'], discipline_cells[\"Unique Failures (FAIL)\"], discipline_cells[\"Unique URLs\"], stats.get('Unique URLs',0))\n",
        "                        set_percentage_formula(ws_summary, discipline_cells['% Total URLs WBDG Warning'], discipline_cells[\"Total WBDG Content Warnings\"], discipline_cells[\"Total URLs\"], stats.get('Total URLs',0))\n",
        "                        set_percentage_formula(ws_summary, discipline_cells['% Unique URLs WBDG Warning'], discipline_cells[\"Unique WBDG Content Warnings\"], discipline_cells[\"Unique URLs\"], stats.get('Unique URLs',0))\n",
        "                        current_row += 1\n",
        "                else: ws_summary.cell(row=current_row, column=1, value=\"No discipline data found.\"); current_row +=1\n",
        "                discipline_end_row = current_row - 1; [[ws_summary.cell(row=r, column=c).border for c in range(1, 3)] for r in range(overall_start_row, overall_end_row + 1)]; [[ws_summary.cell(row=r_idx, column=c_idx_d).border for c_idx_d in range(1, 12)] for r_idx in range(discipline_start_row, discipline_end_row + 1) if current_summary_data_for_excel['By Discipline']]\n",
        "\n",
        "                if current_run_df_urls.empty: df_to_save = pd.DataFrame(columns=output_columns_main)\n",
        "                else:\n",
        "                    for col_main in output_columns_main:\n",
        "                        if col_main not in current_run_df_urls.columns: current_run_df_urls[col_main] = None\n",
        "                    df_to_save = current_run_df_urls[output_columns_main].copy()\n",
        "\n",
        "                df_to_save = df_to_save.fillna('')\n",
        "                df_to_save.to_excel(writer, index=False, sheet_name=main_sheet_name)\n",
        "                ws_main = writer.sheets[main_sheet_name]; ws_main.freeze_panes = \"A2\"\n",
        "                df_file_metadata.to_excel(writer, index=False, sheet_name=file_list_sheet_name); ws_file_list = writer.sheets[file_list_sheet_name]; ws_file_list.freeze_panes = \"A2\"\n",
        "                if not df_file_metadata.empty: table_files = Table(displayName=\"FileListTable\", ref=f\"A1:{get_column_letter(len(output_columns_filelist))}{len(df_file_metadata)+1}\"); table_files.tableStyleInfo = TableStyleInfo(name=\"TableStyleMedium2\", showFirstColumn=False, showLastColumn=False, showRowStripes=True, showColumnStripes=False);_ = [ws_file_list.add_table(table_files) for t in ws_file_list._tables if t.name != table_files.displayName] if not any(t.name == table_files.displayName for t in ws_file_list._tables) else None\n",
        "                ws_spec_list = writer.book.create_sheet(spec_list_sheet_name); ws_spec_list['A1'] = \"UFGS\"; ws_spec_list['B1'] = \"Discipline\"\n",
        "                # Corrected line for writing spec list data\n",
        "                for i, (spec, discipline_val) in enumerate(TARGET_SPECS_DATA):\n",
        "                    ws_spec_list.cell(row=i + 2, column=1, value=spec)\n",
        "                    ws_spec_list.cell(row=i + 2, column=2, value=discipline_val)\n",
        "                ws_spec_list.freeze_panes = \"A2\"\n",
        "                if TARGET_SPECS_DATA: table_spec = Table(displayName=\"SpecListTable\", ref=f\"A1:B{len(TARGET_SPECS_DATA)+1}\"); table_spec.tableStyleInfo = TableStyleInfo(name=\"TableStyleMedium2\", showFirstColumn=False,showLastColumn=False, showRowStripes=True, showColumnStripes=False); _ = [ws_spec_list.add_table(table_spec) for t in ws_spec_list._tables if t.name != table_spec.displayName] if not any(t.name == table_spec.displayName for t in ws_spec_list._tables) else None\n",
        "\n",
        "                red_fill = PatternFill(start_color='FFC7CE',fill_type='solid'); red_font = Font(color='9C0006')\n",
        "                green_fill = PatternFill(start_color='C6EFCE',fill_type='solid'); green_font = Font(color='006100')\n",
        "                orange_fill = PatternFill(start_color='FFD9B3',fill_type='solid'); orange_font = Font(color='A65B00')\n",
        "                grey_fill_error = PatternFill(start_color='D9D9D9',fill_type='solid'); grey_font_error = Font(color='595959')\n",
        "\n",
        "                if len(df_to_save) > 0:\n",
        "                    formatting_range_main = f\"A2:{get_column_letter(len(output_columns_main))}{len(df_to_save) + 1}\"\n",
        "                    try:\n",
        "                        status_col_letter_main = get_column_letter(output_columns_main.index('STATUS') + 1)\n",
        "                        for f_str, font, fill in [(f'${status_col_letter_main}2=\"PASS\"', green_font, green_fill), (f'${status_col_letter_main}2=\"FAIL\"', red_font, red_fill), (f'${status_col_letter_main}2=\"WARN_WBDG_CONTENT_ERROR\"', orange_font, orange_fill), (f'OR(${status_col_letter_main}2=\"PROCESSING_ERROR\", ${status_col_letter_main}2=\"NOT_MAPPED\", ${status_col_letter_main}2=\"BATCH_ITEM_ERROR\", ${status_col_letter_main}2=\"BATCH_LENGTH_ERROR\", ${status_col_letter_main}2=\"BATCH_TYPE_ERROR\", ${status_col_letter_main}2=\"BATCH_CALL_EXCEPTION\", ${status_col_letter_main}2=\"BATCH_MISSING_RESULT\", ${status_col_letter_main}2=\"ASYNC_OVERALL_ERROR\", ${status_col_letter_main}2=\"ERROR_IN_LOOP\", ${status_col_letter_main}2=\"FAIL_WORKER\")', grey_font_error, grey_fill_error)]: ws_main.conditional_formatting.add(formatting_range_main, FormulaRule(formula=[f_str], stopIfTrue=False, font=font, fill=fill))\n",
        "                    except ValueError: debug_print(\"Error finding 'STATUS' column for conditional formatting.\")\n",
        "                if output_columns_main and len(df_to_save) > 0 : table_main = Table(displayName=\"URLCheckTable\", ref=f\"A1:{get_column_letter(len(output_columns_main))}{len(df_to_save) + 1}\"); table_main.tableStyleInfo = TableStyleInfo(name=\"TableStyleLight9\", showFirstColumn=False, showLastColumn=False, showRowStripes=True, showColumnStripes=False); _ = [ws_main.add_table(table_main) for t in ws_main._tables if t.name != table_main.displayName] if not any(t.name == table_main.displayName for t in ws_main._tables) else None\n",
        "                auto_fit_columns(ws_summary, columns_to_fit=['A', 'B']); auto_fit_columns(ws_main); auto_fit_columns(ws_file_list); auto_fit_columns(ws_spec_list)\n",
        "                desired_order = [summary_sheet_name, main_sheet_name, file_list_sheet_name, spec_list_sheet_name]\n",
        "                current_sheet_titles = [s.title for s in writer.book._sheets]; writer.book._sheets = sorted(writer.book._sheets, key=lambda s: (desired_order.index(s.title) if s.title in desired_order else len(desired_order))) if all(sn in current_sheet_titles for sn in desired_order) else writer.book._sheets\n",
        "                debug_print(f\"[{run_label}] Excel file created: {run_specific_excel_path}\")\n",
        "        except Exception as e_excel: print(f\"[{run_label}] Excel saving error: {e_excel}\"); traceback.print_exc()\n",
        "\n",
        "    final_proc_time = time.time() - script_start_time\n",
        "    print(f\"\\n[{run_label}] Total Processing Time: {time.strftime('%H:%M:%S', time.gmtime(final_proc_time))}\")\n",
        "    print(f\"\\n[{run_label}] URL Validation Issue Summary (from error_counts dict for this run):\")\n",
        "    if current_run_error_counts:\n",
        "        sorted_errors = sorted(current_run_error_counts.items(), key=lambda item: (-item[1], item[0]))\n",
        "        for url_key, count_val in sorted_errors:\n",
        "            err_tuple = current_run_validation_results_map.get(url_key)\n",
        "            if err_tuple and isinstance(err_tuple, tuple) and len(err_tuple) == 6:\n",
        "                 status_v, code_v, _, msg_v, _, cert_v = err_tuple\n",
        "                 print(f\"  - Cnt: {count_val}, Status: {status_v}, Code: {code_v}, URL: {url_key[:70]}..., Msg: {msg_v}, Cert: {cert_v}\")\n",
        "            else:\n",
        "                 status_val = err_tuple[0] if err_tuple and isinstance(err_tuple, tuple) and len(err_tuple) > 0 else \"N/A\"\n",
        "                 msg_val = err_tuple[3] if err_tuple and isinstance(err_tuple, tuple) and len(err_tuple) > 3 else \"N/A\"\n",
        "                 print(f\"  - Cnt: {count_val}, Status: {status_val}, URL: {url_key[:70]}... (Detail limited: {msg_val})\")\n",
        "    else:\n",
        "        total_urls_for_validation = len(unique_primary_urls_for_batch_collector) + len(email_urls_for_map_collector) + len(invalid_urls_for_map_collector)\n",
        "        print(f\"  [{run_label}] No URL validation issues recorded in error_counts for this run.\" if total_urls_for_validation > 0 else f\"  [{run_label}] URL validation skipped (no valid URLs found).\")\n",
        "    print(f\"\\n[{run_label}] Analysis complete. Output (if saved): {run_specific_excel_path if save_excel else 'Excel saving disabled for this run.'}\")\n",
        "\n",
        "    return current_run_df_urls\n",
        "\n",
        "async def run_concurrency_comparison_test():\n",
        "    if not async_components_imported_successfully:\n",
        "        print(\"CRITICAL: Cannot run comparison test as async components from 'reachable' failed to import.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n--- Starting Concurrency Comparison Test ---\")\n",
        "\n",
        "    if not os.path.exists(zip_file_path):\n",
        "        print(\"Initial zip download for comparison test...\")\n",
        "        if not ufgs_download_url or \"REPLACE_WITH_CORRECT_UFGS_ZIP_DOWNLOAD_LINK\" in ufgs_download_url:\n",
        "            print(\"Error: UFGS download URL not set. Exiting test.\"); return\n",
        "        try:\n",
        "            with httpx.Client(verify=False) as client:\n",
        "                with client.stream(\"GET\", ufgs_download_url, follow_redirects=True, timeout=60.0) as response:\n",
        "                    response.raise_for_status()\n",
        "                    with open(zip_file_path, 'wb') as f: [f.write(chunk) for chunk in response.iter_bytes(8192)]\n",
        "            print(f\"Initial download complete: {zip_file_path}\")\n",
        "        except Exception as e: print(f\"Initial download error: {e}\"); return\n",
        "\n",
        "    print(\"\\n--- Running with 1 Worker ---\")\n",
        "    df_run1 = await async_process_all_ufgs_data(run_label=\"1_worker\", concurrency_level_override=1, save_excel=False)\n",
        "\n",
        "    print(\"\\n--- Running with 100 Workers ---\")\n",
        "    df_run2 = await async_process_all_ufgs_data(run_label=\"100_workers_use_cache\", concurrency_level_override=100, save_excel=False)\n",
        "\n",
        "    print(\"\\n\\n--- DataFrame Comparison Results ---\")\n",
        "    if df_run1.empty and df_run2.empty: print(\"Both runs resulted in empty DataFrames. Nothing to compare.\"); return\n",
        "    if df_run1.empty: print(\"DataFrame from 1-worker run is empty. Cannot compare.\"); return\n",
        "    if df_run2.empty: print(\"DataFrame from 100-worker run is empty. Cannot compare.\"); return\n",
        "    if 'PRIMARY_URL' not in df_run1.columns or 'PRIMARY_URL' not in df_run2.columns:\n",
        "        print(\"Error: 'PRIMARY_URL' column missing. Cannot compare.\"); return\n",
        "\n",
        "    compare_cols = ['STATUS', 'RESPONSE_CODE', 'FINAL_URL', 'ERROR_MSG', 'CHECK_CERTAINTY', 'IS_WBDG']\n",
        "    for df_temp in [df_run1, df_run2]:\n",
        "        for col_c in compare_cols:\n",
        "            if col_c not in df_temp.columns: df_temp[col_c] = pd.NA\n",
        "\n",
        "    merged_df = pd.merge(\n",
        "        df_run1[['PRIMARY_URL'] + compare_cols].fillna(\"DF1_NA_FILL\"),\n",
        "        df_run2[['PRIMARY_URL'] + compare_cols].fillna(\"DF2_NA_FILL\"),\n",
        "        on='PRIMARY_URL', how='outer', suffixes=('_1w', '_100w')\n",
        "    )\n",
        "\n",
        "    differences_found = 0\n",
        "    print(f\"Comparing {len(merged_df)} unique URLs found across both runs...\")\n",
        "    for _, row in merged_df.iterrows():\n",
        "        url = row['PRIMARY_URL']\n",
        "        discrepancy_details = []\n",
        "        for col_base in compare_cols:\n",
        "            val1, val100 = row[f\"{col_base}_1w\"], row[f\"{col_base}_100w\"]\n",
        "            s_val1 = str(val1 if not pd.isna(val1) else \"MISSING_IN_RUN1\")\n",
        "            s_val100 = str(val100 if not pd.isna(val100) else \"MISSING_IN_RUN2\")\n",
        "            if s_val1 != s_val100:\n",
        "                discrepancy_details.append(f\"  - {col_base}: 1w='{s_val1}' vs 100w='{s_val100}'\")\n",
        "        if discrepancy_details:\n",
        "            differences_found += 1\n",
        "            print(f\"\\nDiscrepancy for URL: {url}\")\n",
        "            for detail in discrepancy_details: print(detail)\n",
        "\n",
        "    if differences_found == 0: print(\"\\nNo differences found in validation results between 1-worker and 100-worker runs.\")\n",
        "    else: print(f\"\\nFound differences for {differences_found} URLs.\")\n",
        "    print(\"\\n--- Concurrency Comparison Test Finished ---\")\n",
        "\n",
        "# --- Wrapper function to be called from a new Colab cell using await ---\n",
        "async def start_processing_if_configured(concurrency_override=None, run_comparison_test=False):\n",
        "    # Ensure global DO_FULL_PROCESSING is accessible if not passed\n",
        "    global DO_FULL_PROCESSING, async_components_imported_successfully\n",
        "\n",
        "    if DO_FULL_PROCESSING and async_components_imported_successfully:\n",
        "        if run_comparison_test:\n",
        "            await run_concurrency_comparison_test()\n",
        "        else:\n",
        "            await async_process_all_ufgs_data(\n",
        "                run_label=f\"run_concurrency_{concurrency_override if concurrency_override else 'default'}\",\n",
        "                concurrency_level_override=concurrency_override,\n",
        "                save_excel=True\n",
        "            )\n",
        "    else:\n",
        "        if not DO_FULL_PROCESSING:\n",
        "            print(\"DO_FULL_PROCESSING is False. Set to True in the script and re-run this cell if you want to process data.\")\n",
        "        if not async_components_imported_successfully:\n",
        "            print(\"CRITICAL: Async components from 'reachable' were not imported successfully. The main process cannot run.\")\n",
        "\n",
        "print(\"\\nScript definitions complete. In a new cell, run: await start_processing_if_configured()\")\n",
        "print(\"Or for comparison test: await start_processing_if_configured(run_comparison_test=True)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYKZ0Zlnz9aL",
        "outputId": "a4330cb3-0554-49fb-aaf5-18670d5009b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting reachable==0.7.0\n",
            "  Downloading reachable-0.7.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (0.28.1)\n",
            "Collecting fake-useragent\n",
            "  Downloading fake_useragent-2.2.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (5.4.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Collecting brotli (from reachable==0.7.0)\n",
            "  Downloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting playwright (from reachable==0.7.0)\n",
            "  Downloading playwright-1.52.0-py3-none-manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting tldextract (from reachable==0.7.0)\n",
            "  Downloading tldextract-5.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from reachable==0.7.0) (4.67.1)\n",
            "Requirement already satisfied: zstandard in /usr/local/lib/python3.11/dist-packages (from reachable==0.7.0) (0.23.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx) (0.16.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx) (4.13.2)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]->reachable==0.7.0) (4.2.0)\n",
            "Collecting pyee<14,>=13 (from playwright->reachable==0.7.0)\n",
            "  Downloading pyee-13.0.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.11/dist-packages (from playwright->reachable==0.7.0) (3.2.2)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from tldextract->reachable==0.7.0) (2.32.3)\n",
            "Collecting requests-file>=1.4 (from tldextract->reachable==0.7.0)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract->reachable==0.7.0) (3.18.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]->reachable==0.7.0) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]->reachable==0.7.0) (4.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract->reachable==0.7.0) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract->reachable==0.7.0) (2.4.0)\n",
            "Downloading reachable-0.7.0-py3-none-any.whl (12 kB)\n",
            "Downloading fake_useragent-2.2.0-py3-none-any.whl (161 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Brotli-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading playwright-1.52.0-py3-none-manylinux1_x86_64.whl (45.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.1/45.1 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.3.0-py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyee-13.0.0-py3-none-any.whl (15 kB)\n",
            "Downloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Installing collected packages: brotli, pyee, fake-useragent, requests-file, playwright, tldextract, reachable\n",
            "Successfully installed brotli-1.1.0 fake-useragent-2.2.0 playwright-1.52.0 pyee-13.0.0 reachable-0.7.0 requests-file-2.1.0 tldextract-5.3.0\n",
            "Successfully imported async components from 'reachable'.\n",
            "\n",
            "Script definitions complete. In a new cell, run: await start_processing_if_configured()\n",
            "Or for comparison test: await start_processing_if_configured(run_comparison_test=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if DO_FULL_PROCESSING and async_components_imported_successfully:\n",
        "    # This will run the main process with default concurrency\n",
        "    # (50 or 100 based on URL count) and save the Excel output.\n",
        "    await async_process_all_ufgs_data(run_label=\"final_output\", save_excel=True)\n",
        "\n",
        "    # --- Optional: For testing specific concurrency levels ---\n",
        "    # To test with 1 worker (slower, but good for isolating issues):\n",
        "    # print(\"\\n--- RUNNING WITH 1 WORKER FOR TESTING ---\")\n",
        "    # await async_process_all_ufgs_data(run_label=\"1_worker_test\", concurrency_level_override=1, save_excel=True)\n",
        "\n",
        "    # To test with 100 workers (faster, more load):\n",
        "    # print(\"\\n--- RUNNING WITH 100 WORKERS FOR TESTING ---\")\n",
        "    # await async_process_all_ufgs_data(run_label=\"100_workers_test\", concurrency_level_override=100, save_excel=True)\n",
        "\n",
        "else:\n",
        "    if not DO_FULL_PROCESSING:\n",
        "        print(\"DO_FULL_PROCESSING is False. Please set it to True in the main script cell and re-run that cell if you want to process data.\")\n",
        "    if not async_components_imported_successfully:\n",
        "        print(\"CRITICAL: Async components from 'reachable' were not imported successfully (check output of the first cell). The main process cannot run.\")"
      ],
      "metadata": {
        "id": "YFTqaegS2zTv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b52788d-c0cb-46f3-e5e4-4c7186128ac1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- [final_output] Main Async Processing Started at Sat May 31 19:25:47 2025 (Debug: False) ---\n",
            "Reachable library version (main script): 0.7.0\n",
            "httpx library version: 0.28.1\n",
            "[final_output] Download complete: /content/UFGS.zip\n",
            "[final_output] Found 700 .SEC files to process.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fake_useragent:Error occurred during getting browser(s): random, but was suppressed with fallback.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[final_output] Prepared 544 URLs for async batch validation.\n",
            "[final_output] Starting async validation (Concurrency: 100, using library default timeouts)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 544/544 [00:22<00:00, 24.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[final_output] Async URL Validation finished in 56.37s.\n",
            "[final_output] Processing 544 async results...\n",
            "\n",
            "[final_output] Total Processing Time: 00:00:59\n",
            "\n",
            "[final_output] URL Validation Issue Summary (from error_counts dict for this run):\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: http://database.ul.com..., Msg: Failed. Error: ReadError. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: http://energy.navy.mil..., Msg: Failed. Error: ConnectionError. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: http://schulerline.com/cuba-liner-service..., Msg: Failed. Error: ConnectionError. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: http://www.aashtoresource.org/aap/overview..., Msg: Failed. Error: ConnectionError. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: http://www.aec.army.mil/services/conserve/pestmanagement.aspx..., Msg: Failed. Error: ConnectionError. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: http://www.aham.org..., Msg: Failed. Error: Max depth reached. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: http://www.aisc.org..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: http://www.dia.mil..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: http://www.dla.mil..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: http://www.dtic.mil/dtic..., Msg: Failed. Error: ConnectionError. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: http://www.dtic.mil/dtic/pdf/customer/STINFOdata/DD14231.pdf..., Msg: Failed. Error: ConnectionError. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: http://www.eere.energy.gov/femp/procurement..., Msg: Failed. Error: ConnectionError. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 404, URL: http://www.energystar.gov/products/certified-products/detail/roof-prod..., Msg: Failed (success=False). (Raw Status: 404), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 404, URL: http://www.epa.gov/ozone/snap/refrigerants/lists/index.html..., Msg: Failed (success=False). (Raw Status: 404), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: http://www.erdc.usace.army.mil/Media/FactSheets/FactSheetArticleView/t..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 404, URL: http://www.faa.gov/arp/pdf/534553ad.pdf..., Msg: Failed (success=False). (Raw Status: 404), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: http://www.geosociety.org..., Msg: Failed. Error: ConnectTimeout. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: http://www.hnd.usace.army.mil/stddgn/redirecttype3.aspx..., Msg: Failed. Error: ConnectTimeout. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: http://www.hnd.usace.army.mil/stddgn/redirecttype4.aspx..., Msg: Failed. Error: ConnectTimeout. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: http://www.iccsafe.org/cs/igcc/pages/default.aspx..., Msg: Failed. Error: Max depth reached. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: http://www.mrca.org..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: http://www.navfac.navy.mil..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: http://www.nsf.org..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: http://www.oecd.org..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: http://www.osha.gov/dep/greenjobs/weather_ventilation.html..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 404, URL: http://www.pbmdf.com/CPA30/files/ccLibraryFiles/FILENAME/000000000416/..., Msg: Failed (success=False). (Raw Status: 404). Domain may be parked., Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: http://www.publications.usace.army.mil..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: http://www.publications.usace.army.mil/Portals/76/Publications/Enginee..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: http://www.tricare.mil/CMAC/home.aspx..., Msg: Failed. Error: RemoteProtocolError. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: WARN_WBDG_CONTENT_ERROR, Code: 404, URL: http://www.wbdg.org/ccb/DOD/UFC/INACTIVE/ufc_3_450_02.pdf..., Msg: WBDG: Content check triggered an error page., Cert: HIGH\n",
            "  - Cnt: 1, Status: WARN_WBDG_CONTENT_ERROR, Code: 404, URL: http://www.wbdg.org/ccb/DOD/UFC/INACTIVE/ufc_3_540_02n.pdf..., Msg: WBDG: Content check triggered an error page., Cert: HIGH\n",
            "  - Cnt: 1, Status: WARN_WBDG_CONTENT_ERROR, Code: 404, URL: http://www.wbdg.org/ccb/DOD/UFC/ufc_3_110_03.pdf..., Msg: WBDG: Content check triggered an error page., Cert: HIGH\n",
            "  - Cnt: 1, Status: WARN_WBDG_CONTENT_ERROR, Code: 404, URL: http://www.wbdg.org/ccb/DOD/UFC/ufc_3_550_01.pdf..., Msg: WBDG: Content check triggered an error page., Cert: HIGH\n",
            "  - Cnt: 1, Status: WARN_WBDG_CONTENT_ERROR, Code: 404, URL: http://www.wbdg.org/ccb/DOD/UFC/ufc_4_010_01.pdf..., Msg: WBDG: Content check triggered an error page., Cert: HIGH\n",
            "  - Cnt: 1, Status: WARN_WBDG_CONTENT_ERROR, Code: 404, URL: http://www.wbdg.org/ccb/NAVGRAPH/graphtoc.pdf..., Msg: WBDG: Content check triggered an error page., Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: http://www.wwpa.org..., Msg: Failed. Error: ConnectTimeout. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: https//mtc.erdc.dren.mil/contact2.aspx..., Msg: Failed. Error: ValueError. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: https://acousticalsociety.org..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: https://aec.army.mil..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: https://aec.army.mil/index.php?cID=432..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: https://asq.org..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: https://cadbim.usace.army.mil..., Msg: Failed. Error: ConnectionError. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: https://cnrse.cnic.navy.mil/Installations/NS-Guantanamo-Bay..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 404, URL: https://dl.dod.cyber.mil/wp-content/uploads/stigs/pdf/U_STIG_Library-z..., Msg: Failed (success=False). (Raw Status: 404), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: https://dqm.usace.army.mil..., Msg: Failed. Error: ConnectionError. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: https://exwc.navfac.navy.mil..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 404, URL: https://fpinnovations.ca/Pages/index.aspx..., Msg: Failed (success=False). (Raw Status: 404), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: https://highways.dot.gov..., Msg: Failed. Error: ReadTimeout. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: https://hub.navfac.navy.mil/webcenter/faces/oracle/webcenter/page/scop..., Msg: Failed. Error: ConnectionError. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: https://intelshare.intelink.gov/sites/NAVSEA-RMF..., Msg: Failed. Error: ConnectionError. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: https://itrcweb.org/HigherLogic/System/DownloadDocumentFile.ashx?Docum..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: https://ncc.navfac.navy.mil/Popular-Links/DOWNLOADS..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 404, URL: https://ndep.nv.gov/uploads/water-nonpoint-docs/NVBMPHandbook1994.pdf..., Msg: Failed (success=False). (Raw Status: 404), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: https://ntia.gov..., Msg: Failed. Error: ConnectionError. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: https://pdc.usace.army.mil/library/tr/10-02..., Msg: Failed. Error: ConnectionError. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: https://portal.navfac.mil/go/locks..., Msg: Failed. Error: ConnectionError. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: https://portal.navfac.navy.mil/portal/page/portal/navfac/navfac_forbus..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: https://rms.usace.army.mil..., Msg: Failed. Error: ConnectionError. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: https://static.azdeq.gov/permits/azpdes/cgp_permit.pdf..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: https://www.acq.osd.mil/dpap/dars/dfars/pdf/current/20220101/227_71.pd..., Msg: Failed. Error: ConnectionError. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: https://www.acq.osd.mil/eie/afpmb/docs/standardlists/dd1532-1.xlsm..., Msg: Failed. Error: ConnectionError. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: https://www.aiha.org..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: https://www.aisc.org..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: https://www.buildingsmart.org/standards/bsi-standards/industry-foundat..., Msg: Failed. Error: InvalidURL. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: https://www.cnic.navy.mil/Operations-and-Management/Base-Support/DBIDS..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: https://www.cpsc.gov..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: https://www.dla.mil/DispositionServices/Offers/Disposal/HazardousWaste..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: https://www.e-publishing.af.mil..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: https://www.epa.gov/smm/comprehensive-procurement-\n",
            "guidelines-construc..., Msg: Failed. Error: InvalidURL. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: https://www.epa.gov/smm/comprehensive-procurement-\n",
            "guidelines-construc..., Msg: Failed. Error: InvalidURL. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: https://www.erdc.usace.army.mil/Media/Fact-Sheets/Fact-Sheet-Article-V..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: https://www.esd.whs.mil/Directives/forms/fmo_poc..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: https://www.fcc.gov..., Msg: Failed. Error: ReadTimeout. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: https://www.fema.gov..., Msg: Failed. Error: ReadTimeout. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: https://www.gsaelibrary.gsa.gov/ElibMain/home.do..., Msg: Failed. Error: ConnectTimeout. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: https://www.hnc.usace.army.mil/Missions/Engineering-Directorate/TECHIN..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 404, URL: https://www.idmanagement.gov/approved-products-list..., Msg: Failed (success=False). (Raw Status: 404). Domain may be parked., Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: https://www.iec.ch..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: https://www.med.navy.mil/sites/nmcphc/Pages/Home.aspx..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: https://www.micainsulation.org..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 404, URL: https://www.mpi.net/APL/index..., Msg: Failed (success=False). (Raw Status: 404), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: https://www.nato.int..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: https://www.navfac.navy.mil..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: https://www.navfac.navy.mil/Business-Lines/Design-and-Construction/Abo..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: https://www.navfac.navy.mil/navfac_worldwide/specialty_centers/ncc/abo..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: https://www.navfac.navy.mil/products_and_services/sb/opportunities/gui..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: https://www.navsea.navy.mil..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: https://www.navsup.navy.mil..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: https://www.navy.mil..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: https://www.nda4u.com..., Msg: Failed. Error: ConnectionError. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: https://www.necanet.org..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: https://www.nps.gov/tps/standards/four-treatments/treatment-\n",
            "rehabilit..., Msg: Failed. Error: InvalidURL. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: https://www.nwo.usace.army.mil/About/Centers-of-Expertise/Protective-D..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: https://www.plasticpipe.org/common/Uploaded%20files/\n",
            "1-PPI/Divisions/M..., Msg: Failed. Error: InvalidURL. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: https://www.publications.usace.army.mil/Portals/76/Users/182/86/2486/E..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: https://www.publications.usace.army.mil/USACE-Publications/Engineer-Re..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: https://www.rd.usda.gov/about-rd/agencies/rural-utilities-service..., Msg: Failed. Error: ReadTimeout. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: https://www.sam.usace.army.mil/Missions/Spatial-Data-Branch/Dredging-Q..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 403, URL: https://www.semi.org..., Msg: Failed (success=False). (Raw Status: 403), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: https://www.transportation.gov..., Msg: Failed. Error: ReadTimeout. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: WARN_WBDG_CONTENT_ERROR, Code: 404, URL: https://www.wbdg.org/FFC/ARMYCOE/usace_airleakagetestprotocol.pdf..., Msg: WBDG: Content check triggered an error page., Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: https://www.wbdg.org/dod/ufgs/\n",
            "forms-graphics-tables..., Msg: Failed. Error: InvalidURL. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: WARN_WBDG_CONTENT_ERROR, Code: 404, URL: https://www.wbdg.org/dod/uggs/ufgs-01-78-24-00-20..., Msg: WBDG: Content check triggered an error page., Cert: HIGH\n",
            "  - Cnt: 1, Status: WARN_WBDG_CONTENT_ERROR, Code: 404, URL: https://www.wbdg.org/ffc/dod/supplemental-technical-criteria/tsewg-tp-..., Msg: WBDG: Content check triggered an error page., Cert: HIGH\n",
            "  - Cnt: 1, Status: WARN_WBDG_CONTENT_ERROR, Code: 404, URL: https://www.wbdg.org/ffc/dod/supplemental-technical-criteria/tsewg-tp-..., Msg: WBDG: Content check triggered an error page., Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: 404, URL: https://www.whitehouse.gov/administration/eop/ceq..., Msg: Failed (success=False). (Raw Status: 404), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: www.ahridirectory.org..., Msg: Failed. Error: ValueError. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: www.airfieldasphaltcert.com..., Msg: Failed. Error: ValueError. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: www.aiswcd.org..., Msg: Failed. Error: ValueError. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: www.biopreferred.gov..., Msg: Failed. Error: ValueError. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: www.cdc.gov..., Msg: Failed. Error: UnsupportedProtocol. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: www.chlor-rid.com..., Msg: Failed. Error: ValueError. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: www.eere.energy.gov/femp/procurement..., Msg: Failed. Error: ValueError. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: www.energystar.gov/products..., Msg: Failed. Error: ValueError. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: www.epa.gov/cpg/products/paint.htm..., Msg: Failed. Error: ValueError. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: www.esd.whs.mil/Portals/54/Documents/DD/forms/dd/dd1354.pdf..., Msg: Failed. Error: ValueError. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: www.idmanagement.gov..., Msg: Failed. Error: ValueError. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: www.in.gov/idem..., Msg: Failed. Error: ValueError. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: www.mhi.org/ecma..., Msg: Failed. Error: ValueError. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: www.nih.gov..., Msg: Failed. Error: ValueError. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: www.osha.gov..., Msg: Failed. Error: ValueError. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: www.pay.gov..., Msg: Failed. Error: ValueError. (Raw Status: -1), Cert: HIGH\n",
            "  - Cnt: 1, Status: FAIL, Code: None, URL: www.publications.usace.army.mil/LinkClick.aspx?fileticket=2t7j4pDfSiA%..., Msg: Failed. Error: ValueError. (Raw Status: -1), Cert: HIGH\n",
            "\n",
            "[final_output] Analysis complete. Output (if saved): /content/SEC_URL_references_with_certainty_final_output.xlsx\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlsD1wyTgqXXCidu5+cfpu",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}